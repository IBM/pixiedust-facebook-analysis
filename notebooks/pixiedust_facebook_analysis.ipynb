{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n",
    "# not use this file except in compliance with the License. You may obtain\n",
    "# a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
    "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
    "# License for the specific language governing permissions and limitations\n",
    "# under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analyze Facebook Data Using IBM Watson and IBM Data Platform\n",
    "\n",
    "This is a three-part notebook written in `Python_3.5` meant to show how anyone can enrich and analyze a combined dataset of unstructured and structured information with IBM Watson and IBM Data Platform. For this example we are using a standard Facebook Analytics export which features texts from posts, articles and thumbnails, along with standard performance metrics such as likes, shares, and impressions. \n",
    "\n",
    "**Part I** will use the Natual Language Understanding, Visual Recognition and Tone Analyzer Services from IBM Watson to enrich the Facebook Posts, Thumbnails, and Articles by pulling out `Emotion Tones`, `Social Tones`, `Language Tones`, `Entities`, `Keywords`, and `Document Sentiment`. The end result of Part I will be additional features and metrics we can test, analyze, and visualize in Part III. \n",
    "\n",
    "**Part II** will be used to set up the visualizations and tests we will run in Part III. The end result of Part II will be multiple pandas DataFrames that will contain the values, and metrics needed to find insights from the Part III tests and experiments.\n",
    "\n",
    "**Part III** will include services from IBM's Data Platform, including IBM's own data visualization library PixieDust. In Part III we will run analysis on the data from the Facebook Analytics export, such as the number of likes, comments, shares, to the overall reach for each post, and will compare it to the enriched data we pulled in Part I.\n",
    "\n",
    "\n",
    "#### You should only need to change data in the Setup portion of this notebook. All places where you see  <span style=\"color: red\"> User Input </span> is where you should be adding inputs. \n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "### [**Part I - Enrich**](#part1)<br>\n",
    "1. [Setup](#setup)<br>\n",
    "   1.1 [Install Watson Developer Cloud and BeautifulSoup Packages](#setup1)<br>\n",
    "   1.2 [Install PixieDust](#pixie)<br> \n",
    "   1.3 [Restart Kernel](#restart)<br> \n",
    "   1.4 [Import Packages and Libraries](#setup2)<br>\n",
    "   1.5 [Add Service Credentials From IBM Cloud for Watson Services](#setup3)<br>\n",
    "2. [Load Data](#load)<br>\n",
    "   2.1 [Load Data From SoftLayer's Object Storage as a Pandas DataFrame](#load1)<br>\n",
    "3. [Prepare Data](#prepare)<br>\n",
    "   3.1 [Data Cleansing with Python](#prepare1)<br>\n",
    "   3.2 [Beautiful Soup to Extract Thumbnails and Extented Links](#prepare2)<br>\n",
    "4. [Enrich Data](#enrich)<br>\n",
    "   4.1 [NLU for Post Text](#nlupost)<br>\n",
    "   4.2 [NLU for Thumbnail Text](#nlutn)<br>\n",
    "   4.3 [NLU for Article Text](#nlulink)<br>\n",
    "   4.4 [Tone Analyzer for Post Text](#tonepost)<br>\n",
    "   4.5 [Tone Analyzer for Article Text](#tonearticle)<br>\n",
    "   4.6 [Visual Recognition](#visual)<br>\n",
    "5. [Write Data](#write)<br>\n",
    "   5.1 [Convert DataFrame to new CSV](#write1)<br>\n",
    "   5.2 [Write Data to SoftLayer's Object Storage](#write2)<br>\n",
    "    \n",
    "### [**Part II - Data Preparation**](#part2)<br>\n",
    "1. [Prepare Data](#prepare)<br>\n",
    "   1.1 [Create Multiple DataFrames for Visualizations](#visualizations)<br>\n",
    "   1.2 [Create a Consolidated Tone DataFrame](#tone)<br>\n",
    "   1.3 [Create a Consolidated Keyword DataFrame](#keyword)<br>\n",
    "   1.4 [Create a Consolidated Entity DataFrame](#entity)<br>\n",
    "  \n",
    "### [**Part III - Analyze**](#part3)<br>\n",
    "\n",
    "1. [Setup](#2setup)<br> \n",
    "    1.1 [Assign Variables](#2setup2)<br>\n",
    "2. [Visualize Data](#2visual)<br>\n",
    "    2.1 [Run PixieDust Visualization Library with Display() API](#2visual1)\n",
    "   \n",
    "### Learn more about the technology used:\n",
    "\n",
    "* [Natual Language Understanding](https://www.ibm.com/watson/developercloud/natural-language-understanding.html)\n",
    "* [Tone Analyzer](https://www.ibm.com/watson/developercloud/tone-analyzer.html)\n",
    "* [Visual Recognition](https://www.ibm.com/watson/services/visual-recognition/)\n",
    "* [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [PixieDust](https://github.com/ibm-cds-labs/pixiedust) (Part III)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Documents\n",
    "[Sample Facebook Posts](https://github.com/IBM/pixiedust-facebook-analysis/blob/master/data/example_input/example_facebook_data.csv) - This is a sample export of IBM Watson's Facebook Page. Engagement metrics such as clicks, impressions, etc. are all changed and do not reflect any actual post performance data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "#  Part I - Enrich\n",
    "\n",
    "<a id=\"setup\"></a>\n",
    "## 1. Setup\n",
    "<a id=\"setup1\"></a>\n",
    "### 1.1 Install Latest Watson Developer Cloud and Beautiful Soup Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade watson-developer-cloud==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pixie\"></a>\n",
    "### 1.2 Install PixieDust Library\n",
    "This notebook provides an overview of how to use the PixieDust library to analyze and visualize various data sets. If you are new to PixieDust or would like to learn more about the library, please go to this [Introductory Notebook](https://apsportal.ibm.com/exchange/public/entry/view/5b000ed5abda694232eb5be84c3dd7c1) or visit the [PixieDust Github](https://ibm-cds-labs.github.io/pixiedust/). The `Setup` section for this notebook uses instructions from the [Intro To PixieDust](https://github.com/ibm-cds-labs/pixiedust/blob/master/notebook/Intro%20to%20PixieDust.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you are running the latest version of PixieDust by running the following cell. Do not run this cell if you installed PixieDust locally from source and want to continue to run PixieDust from source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user --upgrade pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"restart\"></a>\n",
    "### 1.3 Restart Kernel\n",
    "> Required after installs/upgrades only.\n",
    "\n",
    "If any libraries were just installed or upgraded, <span style=\"color: red\">restart the kernel</span> before continuing. After this has been done once, you might want to comment out the `!pip install` lines above for cleaner output and a faster \"Run All\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup2\"></a>\n",
    "### 1.4 Import Packages and Libraries\n",
    "> Tip: To check if you have a package installed, open a new cell and write: `help(<package-name>)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "import watson_developer_cloud\n",
    "from watson_developer_cloud import ToneAnalyzerV3, VisualRecognitionV3\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 import Features, EntitiesOptions, KeywordsOptions, EmotionOptions, SentimentOptions\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from operator import itemgetter\n",
    "from os.path import join, dirname\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pixiedust\n",
    "\n",
    "# Suppress some pandas warnings\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# Suppress SSL warnings\n",
    "requests.packages.urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup3'></a>\n",
    "### 1.5 Add Service Credentials From IBM Cloud for Watson Services\n",
    "Edit the following cell to provide your credentials for Watson Visual Recognition, Natural Language Understanding and Tone Analyzer.\n",
    "\n",
    "###  <span style=\"color: red\"> _User Input_</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "\n",
    "# Watson Visual Recognition\n",
    "VISUAL_RECOGNITION_API_KEY = '<add_vr_api_key>'\n",
    "\n",
    "# Watson Natural Launguage Understanding (NLU)\n",
    "NATURAL_LANGUAGE_UNDERSTANDING_USERNAME = '<add_nlu_username>'\n",
    "NATURAL_LANGUAGE_UNDERSTANDING_PASSWORD = '<add_nlu_password>'\n",
    "\n",
    "# Watson Tone Analyzer\n",
    "TONE_ANALYZER_USERNAME = '<add_tone_analyzer_username>'\n",
    "TONE_ANALYZER_PASSWORD = '<add_tone_analyzer_password>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the Watson clients\n",
    "\n",
    "nlu = watson_developer_cloud.NaturalLanguageUnderstandingV1(version='2017-02-27',\n",
    "                                                            username=NATURAL_LANGUAGE_UNDERSTANDING_USERNAME,\n",
    "                                                            password=NATURAL_LANGUAGE_UNDERSTANDING_PASSWORD)\n",
    "tone_analyzer = ToneAnalyzerV3(version='2016-05-19',\n",
    "                               username=TONE_ANALYZER_USERNAME,\n",
    "                               password=TONE_ANALYZER_PASSWORD)\n",
    "\n",
    "visual_recognition = VisualRecognitionV3('2016-05-20', api_key=VISUAL_RECOGNITION_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load'></a> \n",
    "## 2. Load Data\n",
    "\n",
    "### 2.1 Load data from Object Storage\n",
    "\n",
    "**Select the cell below and place your cursor on an empty line below the comment.** Load the CSV file you want to enrich by clicking on the 10/01 icon (upper right), then click `Insert to code` under the file you want to enrich, and choose `Insert Pandas DataFrame`.\n",
    "\n",
    "Note: There is an issue (https://github.com/IBM/pixiedust-facebook-analysis/issues/39)\n",
    " that causes failure of non utf-8 encodings that requires a workaround.\n",
    " You would fix this in the cell below by adding an encoding parameter to read_csv(). For our example_facebook_data.csv:\n",
   "\n",
   "  `df_data_1 = pd.read_csv(body, encoding='latin-1')` ",
   "\n",
    "###  <span style=\"color: red\"> _User Input_</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure this uses the variable above. The number will vary in the inserted code.\n",
    "try:\n",
    "    df = df_data_1\n",
    "except NameError as e:\n",
    "    print('Error: Setup is incorrect or incomplete.\\n')\n",
    "    print('Follow the instructions to insert the pandas DataFrame above, and edit to')\n",
    "    print('make the generated df_data_# variable match the variable used here.')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the cell below and place your cursor on an empty line below the comment.** \n",
    "Put in the credentials for the file you want to enrich by clicking on the 10/01 (upper right), then click `Insert to code` under the file you want to enrich, and choose `Insert Credentials`.\n",
    "\n",
    "**Change the inserted variable name to `credentials_1`**\n",
    "\n",
    "###  <span style=\"color: red\"> _User Input_</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#insert credentials for file - Change to credentials_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure this uses the variable above. The number will vary in the inserted code.\n",
    "try:\n",
    "    credentials = credentials_1\n",
    "except NameError as e:\n",
    "    print('Error: Setup is incorrect or incomplete.\\n')\n",
    "    print('Follow the instructions to insert the file credentials above, and edit to')\n",
    "    print('make the generated credentials_# variable match the variable used here.')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prepare'></a>\n",
    "## 3. Prepare Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prepare1'></a>\n",
    "###  3.1 Data Cleansing with Python\n",
    "Renaming columns, removing noticeable noise in the data, pulling out URLs and appending to a new column to run through NLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'Post Message': 'Text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop the rows that have NaN for the text.\n",
    "df.dropna(subset=['Text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_http = df[\"Text\"].str.partition(\"http\")\n",
    "df_www = df[\"Text\"].str.partition(\"www\")\n",
    "\n",
    "# Combine delimiters with actual links\n",
    "df_http[\"Link\"] = df_http[1].map(str) + df_http[2]\n",
    "df_www[\"Link1\"] = df_www[1].map(str) + df_www[2]\n",
    "\n",
    "# Include only Link columns\n",
    "df_http.drop(df_http.columns[0:3], axis=1, inplace = True)\n",
    "df_www.drop(df_www.columns[0:3], axis=1, inplace = True)\n",
    "\n",
    "# Merge http and www DataFrames\n",
    "dfmerge = pd.concat([df_http, df_www], axis=1)\n",
    "\n",
    "# The following steps will allow you to merge data columns from the left to the right\n",
    "dfmerge = dfmerge.apply(lambda x: x.str.strip()).replace('', np.nan)\n",
    "\n",
    "# Use fillna to fill any blanks with the Link1 column\n",
    "dfmerge[\"Link\"].fillna(dfmerge[\"Link1\"], inplace = True)\n",
    "\n",
    "# Delete Link1 (www column)\n",
    "dfmerge.drop(\"Link1\", axis=1, inplace = True)\n",
    "\n",
    "# Combine Link data frame\n",
    "df = pd.concat([dfmerge,df], axis = 1)\n",
    "\n",
    "# Make sure text column is a string\n",
    "df[\"Text\"] = df[\"Text\"].astype(\"str\")\n",
    "\n",
    "# Strip links from Text column\n",
    "df['Text'] = df['Text'].apply(lambda x: x.split('http')[0])\n",
    "df['Text'] = df['Text'].apply(lambda x: x.split('www')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull thumbnail descriptions and image URLs using requests and beautiful soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change links from objects to strings\n",
    "for link in df.Link:\n",
    "    df.Link.to_string()\n",
    "\n",
    "piclinks = []\n",
    "description = []\n",
    "for url in df[\"Link\"]:\n",
    "    if pd.isnull(url):\n",
    "        piclinks.append(\"\")\n",
    "        description.append(\"\")\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        # Skip certificate check with verify=False.\n",
    "        # Don't do this if your urls are not secure.\n",
    "        page3 = requests.get(url, verify=False)\n",
    "        if page3.status_code != requests.codes.ok:\n",
    "            piclinks.append(\"\")\n",
    "            description.append(\"\")\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(\"Skipping url %s: %s\" % (url, e))\n",
    "        piclinks.append(\"\")\n",
    "        description.append(\"\")\n",
    "        continue\n",
    "        \n",
    "    soup3 = bs(page3.text,\"lxml\")\n",
    "    \n",
    "    pic = soup3.find('meta', property =\"og:image\")\n",
    "    if pic:\n",
    "        piclinks.append(pic[\"content\"])\n",
    "    else: \n",
    "        piclinks.append(\"\")\n",
    "    \n",
    "    content = None\n",
    "    desc = soup3.find(attrs={'name':'Description'})\n",
    "    if desc:\n",
    "        content = desc['content']\n",
    "    if not content or content == 'null':\n",
    "        # Try again with lowercase description\n",
    "        desc = soup3.find(attrs={'name':'description'})\n",
    "        if desc:\n",
    "            content = desc['content']\n",
    "    if not content or content == 'null':\n",
    "        description.append(\"\")\n",
    "    else:\n",
    "        description.append(content)\n",
    "            \n",
    "# Save thumbnail descriptions to df in a column titled 'Thumbnails'\n",
    "df[\"Thumbnails\"] = description\n",
    "# Save image links to df in a column titled 'Image'\n",
    "df[\"Image\"] = piclinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert shortened links to full links.\n",
    "Use requests module to pull extended links. This is only necessary if the Facebook page uses different links than the articles themselves. For this example we are using IBM Watson's Facebook export which uses an IBM link. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortlink = df[\"Link\"]\n",
    "extendedlink = []\n",
    "\n",
    "for link in shortlink:\n",
    "    if isinstance(link, float):  # Float is not a URL, probably NaN.\n",
    "        extendedlink.append('')\n",
    "    else:\n",
    "        try:\n",
    "            extended_link = requests.Session().head(link, allow_redirects=True).url\n",
    "            extendedlink.append(extended_link)\n",
    "        except Exception as e:\n",
    "            print(\"Skipping link %s: %s\" % (link, e))\n",
    "            extendedlink.append('')\n",
    "\n",
    "df[\"Extended Links\"] = extendedlink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='enrich'></a> \n",
    "## 4. Enrichment Time!\n",
    "<a id='nlupost'></a>\n",
    "###  4.1 NLU for the Post Text\n",
    "Below uses Natural Language Understanding to iterate through each post and extract the enrichment features we want to use in our future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of features to get enrichment values for entities, keywords, emotion and sentiment\n",
    "features = Features(entities=EntitiesOptions(), keywords=KeywordsOptions(), emotion=EmotionOptions(), sentiment=SentimentOptions())\n",
    "\n",
    "overallSentimentScore = []\n",
    "overallSentimentType = []\n",
    "highestEmotion = []\n",
    "highestEmotionScore = []\n",
    "kywords = []\n",
    "entities = []\n",
    "\n",
    "# Go through every response and enrich the text using NLU.\n",
    "for text in df['Text']:\n",
    "    # We are assuming English to avoid errors when the language cannot be detected.\n",
    "    enriched_json = nlu.analyze(text=text, features=features, language='en')\n",
    "\n",
    "    # Get the SENTIMENT score and type\n",
    "    if 'sentiment' in enriched_json:\n",
    "        if('score' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentScore.append(enriched_json[\"sentiment\"][\"document\"][\"score\"])\n",
    "        else:\n",
    "            overallSentimentScore.append('0')\n",
    "\n",
    "        if('label' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentType.append(enriched_json[\"sentiment\"][\"document\"][\"label\"])\n",
    "        else:\n",
    "            overallSentimentType.append('0')\n",
    "    else:\n",
    "        overallSentimentScore.append('0')\n",
    "        overallSentimentType.append('0')\n",
    "\n",
    "    # Read the EMOTIONS into a dict and get the key (emotion) with maximum value\n",
    "    if 'emotion' in enriched_json:\n",
    "        me = max(enriched_json[\"emotion\"][\"document\"][\"emotion\"].items(), key=operator.itemgetter(1))[0]\n",
    "        highestEmotion.append(me)\n",
    "        highestEmotionScore.append(enriched_json[\"emotion\"][\"document\"][\"emotion\"][me])\n",
    "    else:\n",
    "        highestEmotion.append(\"\")\n",
    "        highestEmotionScore.append(\"\")\n",
    "\n",
    "    # Iterate and get KEYWORDS with a confidence of over 70%\n",
    "    if 'keywords' in enriched_json:\n",
    "        tmpkw = []\n",
    "        for kw in enriched_json['keywords']:\n",
    "            if(float(kw[\"relevance\"]) >= 0.7):\n",
    "                tmpkw.append(kw[\"text\"])\n",
    "        # Convert multiple keywords in a list to a string and append the string\n",
    "        kywords.append(', '.join(tmpkw))\n",
    "    else:\n",
    "        kywords.append(\"\")\n",
    "            \n",
    "    # Iterate and get Entities with a confidence of over 30%\n",
    "    if 'entities' in enriched_json:\n",
    "        tmpent = []\n",
    "        for ent in enriched_json['entities']: \n",
    "            if(float(ent[\"relevance\"]) >= 0.3):\n",
    "                tmpent.append(ent[\"type\"])\n",
    " \n",
    "        # Convert multiple entities in a list to a string and append the string\n",
    "        entities.append(', '.join(tmpent))\n",
    "    else:\n",
    "        entities.append(\"\")    \n",
    "    \n",
    "# Create columns from the list and append to the DataFrame\n",
    "if highestEmotion:\n",
    "    df['TextHighestEmotion'] = highestEmotion\n",
    "if highestEmotionScore:\n",
    "    df['TextHighestEmotionScore'] = highestEmotionScore\n",
    "\n",
    "if overallSentimentType:\n",
    "    df['TextOverallSentimentType'] = overallSentimentType\n",
    "if overallSentimentScore:\n",
    "    df['TextOverallSentimentScore'] = overallSentimentScore\n",
    "\n",
    "df['TextKeywords'] = kywords\n",
    "df['TextEntities'] = entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we extract all of the Keywords and Entities from each Post, we have columns with multiple Keywords and Entities separated by commas. For our Analysis in Part II, we also wanted the top Keyword and Entity for each Post. Because of this, we added two new columns to capture the `MaxTextKeyword` and `MaxTextEntity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose first of Keywords,Concepts, Entities\n",
    "df[\"MaxTextKeywords\"] = df[\"TextKeywords\"].apply(lambda x: x.split(',')[0])\n",
    "df[\"MaxTextEntity\"] = df[\"TextEntities\"].apply(lambda x: x.split(',')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nlutn'></a>\n",
    "###  4.2 NLU for Thumbnail Text\n",
    "\n",
    "We will repeat the same process for Thumbnails and Article Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of features to get enrichment values for entities, keywords, emotion and sentiment\n",
    "features = Features(entities=EntitiesOptions(), keywords=KeywordsOptions(), emotion=EmotionOptions(), sentiment=SentimentOptions())\n",
    "\n",
    "overallSentimentScore = []\n",
    "overallSentimentType = []\n",
    "highestEmotion = []\n",
    "highestEmotionScore = []\n",
    "kywords = []\n",
    "entities = []\n",
    "\n",
    "# Go through every response and enrich the text using NLU.\n",
    "for text in df['Thumbnails']:\n",
    "    if not text:\n",
    "        overallSentimentScore.append(' ')\n",
    "        overallSentimentType.append(' ')\n",
    "        highestEmotion.append(' ')\n",
    "        highestEmotionScore.append(' ')\n",
    "        kywords.append(' ')\n",
    "        entities.append(' ')\n",
    "        continue\n",
    "\n",
    "    enriched_json = nlu.analyze(text=text, features=features, language='en')\n",
    "\n",
    "    # Get the SENTIMENT score and type\n",
    "    if 'sentiment' in enriched_json:\n",
    "        if('score' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentScore.append(enriched_json[\"sentiment\"][\"document\"][\"score\"])\n",
    "        else:\n",
    "            overallSentimentScore.append(\"\")\n",
    "\n",
    "        if('label' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentType.append(enriched_json[\"sentiment\"][\"document\"][\"label\"])\n",
    "        else:\n",
    "            overallSentimentType.append(\"\")\n",
    "\n",
    "    # Read the EMOTIONS into a dict and get the key (emotion) with maximum value\n",
    "    if 'emotion' in enriched_json:\n",
    "        me = max(enriched_json[\"emotion\"][\"document\"][\"emotion\"].items(), key=operator.itemgetter(1))[0]\n",
    "        highestEmotion.append(me)\n",
    "        highestEmotionScore.append(enriched_json[\"emotion\"][\"document\"][\"emotion\"][me])\n",
    "\n",
    "    else:\n",
    "        highestEmotion.append(\"\")\n",
    "        highestEmotionScore.append(\"\")\n",
    "\n",
    "    # Iterate and get KEYWORDS with a confidence of over 70%\n",
    "    if 'keywords' in enriched_json:\n",
    "        tmpkw = []\n",
    "        for kw in enriched_json['keywords']:\n",
    "            if(float(kw[\"relevance\"]) >= 0.7):\n",
    "                tmpkw.append(kw[\"text\"])\n",
    "        # Convert multiple keywords in a list to a string and append the string\n",
    "        kywords.append(', '.join(tmpkw))\n",
    "     \n",
    "    # Iterate and get Entities with a confidence of over 30%\n",
    "    if 'entities' in enriched_json:\n",
    "        tmpent = []\n",
    "        for ent in enriched_json['entities']:              \n",
    "            if(float(ent[\"relevance\"]) >= 0.3):\n",
    "                tmpent.append(ent[\"type\"])\n",
    "        # Convert multiple entities in a list to a string and append the string\n",
    "        entities.append(', '.join(tmpent))\n",
    "    else:\n",
    "        entities.append(\"\")     \n",
    "  \n",
    "# Create columns from the list and append to the DataFrame\n",
    "if highestEmotion:\n",
    "    df['ThumbnailHighestEmotion'] = highestEmotion\n",
    "if highestEmotionScore:\n",
    "    df['ThumbnailHighestEmotionScore'] = highestEmotionScore\n",
    "\n",
    "if overallSentimentType:\n",
    "    df['ThumbnailOverallSentimentType'] = overallSentimentType\n",
    "if overallSentimentScore:\n",
    "    df['ThumbnailOverallSentimentScore'] = overallSentimentScore\n",
    "\n",
    "df['ThumbnailKeywords'] = kywords\n",
    "df['ThumbnailEntities'] = entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'Max' to first one from keywords and entities lists\n",
    "df[\"MaxThumbnailKeywords\"] = df[\"ThumbnailKeywords\"].apply(lambda x: x.split(',')[0])\n",
    "df[\"MaxThumbnailEntity\"] = df[\"ThumbnailEntities\"].apply(lambda x: x.split(',')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nlulink'></a> \n",
    "### 4.3 NLU for Article Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of features to get enrichment values for entities, keywords, emotion and sentiment\n",
    "features = Features(entities=EntitiesOptions(), keywords=KeywordsOptions(), emotion=EmotionOptions(), sentiment=SentimentOptions())\n",
    "\n",
    "overallSentimentScore = []\n",
    "overallSentimentType = []\n",
    "highestEmotion = []\n",
    "highestEmotionScore = []\n",
    "kywords = []\n",
    "entities = []\n",
    "article_text = []\n",
    "        \n",
    "# Go through every response and enrich the article using NLU\n",
    "for url in df['Extended Links']:\n",
    "    if not url:\n",
    "        overallSentimentScore.append(' ')\n",
    "        overallSentimentType.append(' ')\n",
    "        highestEmotion.append(' ')\n",
    "        highestEmotionScore.append(' ')\n",
    "        kywords.append(' ')\n",
    "        entities.append(' ')\n",
    "        article_text.append(' ')\n",
    "        continue\n",
    "\n",
    "    # Run links through NLU to get entities, keywords, emotion and sentiment.\n",
    "    # Use return_analyzed_text to extract text for Tone Analyzer to use.\n",
    "    enriched_json = nlu.analyze(url=url,\n",
    "                                features=features,\n",
    "                                language='en',\n",
    "                                return_analyzed_text=True)\n",
    "    \n",
    "    article_text.append(enriched_json[\"analyzed_text\"])\n",
    "\n",
    "    # Get the SENTIMENT score and type\n",
    "    if 'sentiment' in enriched_json:\n",
    "        if('score' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentScore.append(enriched_json[\"sentiment\"][\"document\"][\"score\"])\n",
    "        else:\n",
    "            overallSentimentScore.append('None')\n",
    "\n",
    "        if('label' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentType.append(enriched_json[\"sentiment\"][\"document\"][\"label\"])\n",
    "        else:\n",
    "            overallSentimentType.append('')\n",
    "\n",
    "    # Read the EMOTIONS into a dict and get the key (emotion) with maximum value\n",
    "    if 'emotion' in enriched_json:\n",
    "        me = max(enriched_json[\"emotion\"][\"document\"][\"emotion\"].items(), key=operator.itemgetter(1))[0]\n",
    "        highestEmotion.append(me)\n",
    "        highestEmotionScore.append(enriched_json[\"emotion\"][\"document\"][\"emotion\"][me])\n",
    "\n",
    "    else:\n",
    "        highestEmotion.append('')\n",
    "        highestEmotionScore.append('')\n",
    "\n",
    "    # Iterate and get KEYWORDS with a confidence of over 70%\n",
    "    if 'keywords' in enriched_json:\n",
    "        tmpkw = []\n",
    "        for kw in enriched_json['keywords']:\n",
    "            if(float(kw[\"relevance\"]) >= 0.7):\n",
    "                tmpkw.append(kw[\"text\"])\n",
    "        # Convert multiple keywords in a list to a string and append the string\n",
    "        kywords.append(', '.join(tmpkw))\n",
    "    else: \n",
    "        kywords.append(\"\")\n",
    "            \n",
    "    # Iterate and get Entities with a confidence of over 30%\n",
    "    if 'entities' in enriched_json:\n",
    "        tmpent = []\n",
    "        for ent in enriched_json['entities']:               \n",
    "            if(float(ent[\"relevance\"]) >= 0.3):\n",
    "                tmpent.append(ent[\"type\"])\n",
    "        # Convert multiple entities in a list to a string and append the string\n",
    "        entities.append(', '.join(tmpent))\n",
    "    else:\n",
    "        entities.append(\"\")\n",
    "    \n",
    "# Create columns from the list and append to the DataFrame\n",
    "if highestEmotion:\n",
    "    df['LinkHighestEmotion'] = highestEmotion\n",
    "if highestEmotionScore:\n",
    "    df['LinkHighestEmotionScore'] = highestEmotionScore\n",
    "\n",
    "if overallSentimentType:\n",
    "    df['LinkOverallSentimentType'] = overallSentimentType\n",
    "if overallSentimentScore:\n",
    "    df['LinkOverallSentimentScore'] = overallSentimentScore\n",
    "\n",
    "df['LinkKeywords'] = kywords\n",
    "df['LinkEntities'] = entities\n",
    "df['Article Text'] = article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set 'Max' to first one from keywords and entities lists\n",
    "df[\"MaxLinkKeywords\"] = df[\"LinkKeywords\"].apply(lambda x: x.split(',')[0])\n",
    "df[\"MaxLinkEntity\"] = df[\"LinkEntities\"].apply(lambda x: x.split(',')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='tonepost'></a> \n",
    "### 4.4 Tone Analyzer for Post Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highestEmotionTone = []\n",
    "emotionToneScore = []\n",
    "\n",
    "highestLanguageTone = []\n",
    "languageToneScore = []\n",
    "\n",
    "highestSocialTone = []\n",
    "socialToneScore = []\n",
    "\n",
    "for text in df['Text']:\n",
    "    enriched_json = tone_analyzer.tone(text, content_type='text/plain')\n",
    "        \n",
    "    me = max(enriched_json[\"document_tone\"][\"tone_categories\"][0][\"tones\"], key = itemgetter('score'))['tone_name']      \n",
    "    highestEmotionTone.append(me)\n",
    "    you = max(enriched_json[\"document_tone\"][\"tone_categories\"][0][\"tones\"], key = itemgetter('score'))['score']\n",
    "    emotionToneScore.append(you)\n",
    "            \n",
    "    me1 = max(enriched_json[\"document_tone\"][\"tone_categories\"][1][\"tones\"], key = itemgetter('score'))['tone_name']      \n",
    "    highestLanguageTone.append(me1)\n",
    "    you1 = max(enriched_json[\"document_tone\"][\"tone_categories\"][1][\"tones\"], key = itemgetter('score'))['score']\n",
    "    languageToneScore.append(you1)\n",
    "            \n",
    "    me2 = max(enriched_json[\"document_tone\"][\"tone_categories\"][2][\"tones\"], key = itemgetter('score'))['tone_name']      \n",
    "    highestSocialTone.append(me2)\n",
    "    you2 = max(enriched_json[\"document_tone\"][\"tone_categories\"][2][\"tones\"], key = itemgetter('score'))['score']\n",
    "    socialToneScore.append(you2)\n",
    "    \n",
    "df['highestEmotionTone'] = highestEmotionTone    \n",
    "df['emotionToneScore'] = emotionToneScore\n",
    "df['languageToneScore'] = languageToneScore\n",
    "df['highestLanguageTone'] = highestLanguageTone\n",
    "df['highestSocialTone'] = highestSocialTone    \n",
    "df['socialToneScore'] = socialToneScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tonearticle'></a> \n",
    "### 4.5 Tone Analyzer for Article Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike NLU, Tone Analyzer cannot iterate through a URL, so we used NLU (above) to pull the Article Text from the URL and append it to the DataFrame.\n",
    "\n",
    "Now using Tone Analyzer, we iterate through the `Article Text` column which contains all of the free form text from the articles contained in the Facebook posts. \n",
    "\n",
    "We are using Tone Analyzer to gather the top Social, Language and Emotion Tones from the Articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highestEmotionTone = []\n",
    "emotionToneScore = []\n",
    "\n",
    "highestLanguageTone = []\n",
    "languageToneScore = []\n",
    "\n",
    "highestSocialTone = []\n",
    "socialToneScore = []\n",
    "\n",
    "for text in df['Article Text']:\n",
    "    if not text:\n",
    "        emotionToneScore.append(' ')\n",
    "        highestEmotionTone.append(' ')     \n",
    "        languageToneScore.append(' ')\n",
    "        highestLanguageTone.append(' ')     \n",
    "        socialToneScore.append(' ')\n",
    "        highestSocialTone.append(' ')  \n",
    "        continue\n",
    "\n",
    "    enriched_json = tone_analyzer.tone(text, content_type='text/plain', sentences=False)\n",
    "        \n",
    "    maxTone = max(enriched_json[\"document_tone\"][\"tone_categories\"][0][\"tones\"], key = itemgetter('score'))['tone_name']      \n",
    "    highestEmotionTone.append(maxTone)\n",
    "    maxToneScore = max(enriched_json[\"document_tone\"][\"tone_categories\"][0][\"tones\"], key = itemgetter('score'))['score']\n",
    "    emotionToneScore.append(maxToneScore)\n",
    "            \n",
    "    maxLanguageTone = max(enriched_json[\"document_tone\"][\"tone_categories\"][1][\"tones\"], key = itemgetter('score'))['tone_name']      \n",
    "    highestLanguageTone.append(maxLanguageTone)\n",
    "    maxLanguageScore = max(enriched_json[\"document_tone\"][\"tone_categories\"][1][\"tones\"], key = itemgetter('score'))['score']\n",
    "    languageToneScore.append(maxLanguageScore)\n",
    "            \n",
    "    maxSocial = max(enriched_json[\"document_tone\"][\"tone_categories\"][2][\"tones\"], key = itemgetter('score'))['tone_name']      \n",
    "    highestSocialTone.append(maxSocial)\n",
    "    maxSocialScore = max(enriched_json[\"document_tone\"][\"tone_categories\"][2][\"tones\"], key = itemgetter('score'))['score']\n",
    "    socialToneScore.append(maxSocialScore)\n",
    "   \n",
    "df['articlehighestEmotionTone'] = highestEmotionTone\n",
    "df['articleEmotionToneScore'] = emotionToneScore\n",
    "df['articlelanguageToneScore'] = languageToneScore\n",
    "df['articlehighestLanguageTone'] = highestLanguageTone\n",
    "df['articlehighestSocialTone'] = highestSocialTone\n",
    "df['articlesocialToneScore'] = socialToneScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visual'></a> \n",
    "### 4.6 Visual Recognition\n",
    "Below uses Visual Recognition to classify the thumbnail images.\n",
    "\n",
    "> NOTE: When using the **free tier** of Visual Recognition, _classify_ has a limit of 250 images per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piclinks = df[\"Image\"]\n",
    "\n",
    "picclass = []\n",
    "piccolor = []\n",
    "pictype1 = []\n",
    "pictype2 = []\n",
    "pictype3 = []\n",
    "\n",
    "for pic in piclinks:\n",
    "    if not pic or pic == 'default-img':\n",
    "        picclass.append(' ')\n",
    "        piccolor.append(' ')\n",
    "        pictype1.append(' ')\n",
    "        pictype2.append(' ')\n",
    "        pictype3.append(' ')\n",
    "        continue\n",
    "\n",
    "    classes = []\n",
    "    enriched_json = {}\n",
    "    try:\n",
    "        enriched_json = visual_recognition.classify(parameters=json.dumps({'url': pic}))\n",
    "    except Exception as e:\n",
    "        print(\"Skipping url %s: %s\" % (pic, e))\n",
    "\n",
    "    if 'error' in enriched_json:\n",
    "        print(enriched_json['error'])\n",
    "    if 'images' in enriched_json and 'classifiers' in enriched_json['images'][0]:\n",
    "        classes = enriched_json['images'][0][\"classifiers\"][0][\"classes\"]\n",
    "\n",
    "    color1 = None\n",
    "    class1 = None\n",
    "    type_hierarchy1 = None\n",
    "\n",
    "    for iclass in classes:\n",
    "        # Grab the first color, first class, and first type hierarchy.\n",
    "        # Note: Usually you'd filter by 'score' too.\n",
    "        if not type_hierarchy1 and 'type_hierarchy' in iclass:\n",
    "            type_hierarchy1 = iclass['type_hierarchy']\n",
    "        if not class1:\n",
    "            class1 = iclass['class']\n",
    "        if not color1 and iclass['class'].endswith(' color'):\n",
    "            color1 = iclass['class'][:-len(' color')]\n",
    "        if type_hierarchy1 and class1 and color1:\n",
    "            # We are only using 1 of each per image. When we have all 3, break.\n",
    "            break\n",
    "\n",
    "    picclass.append(class1 or ' ')\n",
    "    piccolor.append(color1 or ' ')\n",
    "    type_split = (type_hierarchy1 or '/ / / ').split('/')\n",
    "    pictype1.append(type_split[1] if len(type_split) > 1 else '-')\n",
    "    pictype2.append(type_split[2] if len(type_split) > 2 else '- ')\n",
    "    pictype3.append(type_split[3] if len(type_split) > 3 else '-')\n",
    "\n",
    "df[\"Image Color\"] = piccolor\n",
    "df[\"Image Class\"] = picclass\n",
    "df[\"Image Type\"] = pictype1\n",
    "df[\"Image Subtype\"] = pictype2\n",
    "df[\"Image Subtype2\"] = pictype3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='write'></a>\n",
    "## Enrichment is now COMPLETE!\n",
    "<a id='write1'></a> \n",
    "Save a copy of the enriched DataFrame as a file in IBM Object Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cos = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n",
    "    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n",
    "    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url=credentials['ENDPOINT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the enriched file name from the original filename.\n",
    "localfilename = 'enriched_' + credentials['FILE']\n",
    "\n",
    "# Write a CSV file from the enriched pandas DataFrame.\n",
    "df.to_csv(localfilename, index=False)\n",
    "\n",
    "# Use the above put_file method with credentials to put the file in Object Storage.\n",
    "cos.upload_file(localfilename, Bucket=credentials['BUCKET'],Key=localfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a> \n",
    "# Part II - Data Preparation\n",
    "<a id='prepare'></a>\n",
    "## 1. Prepare Data\n",
    " <a id='visualizations'></a>\n",
    "### 1.1 Prepare Multiple DataFrames for Visualizations\n",
    "Before we can create the separate tables for each Watson feature we need to organize and reformat the data. First, we need to determine which data points are tied to metrics. Second, we need to make sure make sure each metric is numeric. _(This is necessary for PixieDust in Part III)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Determine which data points are tied to metrics and put them in a list\n",
    "metrics = [\"Lifetime Post Total Reach\", \"Lifetime Post organic reach\", \"Lifetime Post Paid Reach\", \"Lifetime Post Total Impressions\", \"Lifetime Post Organic Impressions\", \n",
    "           \"Lifetime Post Paid Impressions\", \"Lifetime Engaged Users\", \"Lifetime Post Consumers\", \"Lifetime Post Consumptions\", \"Lifetime Negative feedback\", \"Lifetime Negative Feedback from Users\", \n",
    "           \"Lifetime Post Impressions by people who have liked your Page\", \"Lifetime Post reach by people who like your Page\", \"Lifetime Post Paid Impressions by people who have liked your Page\", \n",
    "           \"Lifetime Paid reach of a post by people who like your Page\", \"Lifetime People who have liked your Page and engaged with your post\", \"Lifetime Talking About This (Post) by action type - comment\", \n",
    "           \"Lifetime Talking About This (Post) by action type - like\", \"Lifetime Talking About This (Post) by action type - share\", \"Lifetime Post Stories by action type - comment\", \"Lifetime Post Stories by action type - like\", \n",
    "           \"Lifetime Post Stories by action type - share\", \"Lifetime Post consumers by type - link clicks\", \"Lifetime Post consumers by type - other clicks\", \"Lifetime Post consumers by type - photo view\", \"Lifetime Post Consumptions by type - link clicks\", \n",
    "           \"Lifetime Post Consumptions by type - other clicks\", \"Lifetime Post Consumptions by type - photo view\", \"Lifetime Negative feedback - hide_all_clicks\", \"Lifetime Negative feedback - hide_clicks\", \n",
    "           \"Lifetime Negative Feedback from Users by Type - hide_all_clicks\", \"Lifetime Negative Feedback from Users by Type - hide_clicks\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tone'></a> \n",
    "### 1.2 Create a Consolidated Tone DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Tone DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list with only Post Tone Values\n",
    "post_tones = [\"Text\",\"highestEmotionTone\", \"emotionToneScore\", \"languageToneScore\", \"highestLanguageTone\", \"highestSocialTone\", \"socialToneScore\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "post_tones.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with tones and metrics\n",
    "df_post_tones = df[post_tones]\n",
    "\n",
    "# Determine which tone values are suppose to be numeric and ensure they are numeric. \n",
    "post_numeric_values = [\"emotionToneScore\", \"languageToneScore\", \"socialToneScore\"]\n",
    "for i in post_numeric_values:\n",
    "    df_post_tones[i] = pd.to_numeric(df_post_tones[i], errors='coerce')\n",
    "\n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_post_tones[i] = pd.to_numeric(df_post_tones[i], errors='coerce')\n",
    "    \n",
    "# Drop NA Values in Tone Enrichment Columns\n",
    "df_post_tones.dropna(subset=[\"socialToneScore\"] , inplace = True)\n",
    "\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_post_tones[\"Type\"] = \"Post\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Article Tone DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list with only Article Tone Values\n",
    "article_tones = [\"Text\", \"articlehighestEmotionTone\", \"articleEmotionToneScore\", \"articlelanguageToneScore\", \"articlehighestLanguageTone\", \"articlehighestSocialTone\", \"articlesocialToneScore\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "article_tones.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with tones and metrics\n",
    "df_article_tones = df[article_tones]\n",
    "\n",
    "# Determine which values are suppose to be numeric and ensure they are numeric. \n",
    "art_numeric_values = [\"articleEmotionToneScore\", \"articlelanguageToneScore\", \"articlesocialToneScore\"]\n",
    "for i in art_numeric_values:\n",
    "    df_article_tones[i] = pd.to_numeric(df_article_tones[i], errors='coerce')\n",
    "    \n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_article_tones[i] = pd.to_numeric(df_article_tones[i], errors='coerce')\n",
    "    \n",
    "# Drop NA Values in Tone Enrichment Columns\n",
    "df_article_tones.dropna(subset=[\"articlesocialToneScore\"] , inplace = True)\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_article_tones[\"Type\"] = \"Article\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Post and Article DataFrames to Make One Tone DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First make the Column Headers the same\n",
    "df_post_tones.rename(columns={\"highestEmotionTone\":\"Emotion Tone\", \"emotionToneScore\":\"Emotion Tone Score\", \"languageToneScore\": \"Language Tone Score\", \"highestLanguageTone\": \"Language Tone\", \"highestSocialTone\": \"Social Tone\", \"socialToneScore\":\"Social Tone Score\"\n",
    "}, inplace=True)\n",
    "\n",
    "df_article_tones.rename(columns={\"articlehighestEmotionTone\":\"Emotion Tone\", \"articleEmotionToneScore\":\"Emotion Tone Score\", \"articlelanguageToneScore\": \"Language Tone Score\", \"articlehighestLanguageTone\": \"Language Tone\", \"articlehighestSocialTone\": \"Social Tone\", \"articlesocialToneScore\":\"Social Tone Score\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Combine into one data frame\n",
    "df_tones = pd.concat([df_post_tones, df_article_tones])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='keyword'></a> \n",
    "### 1.3 Create a Consolidated Keyword DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Article Keyword DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list with only Article Keywords\n",
    "article_keywords = [\"Text\", \"MaxLinkKeywords\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "article_keywords.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with keywords and metrics\n",
    "df_article_keywords = df[article_keywords]\n",
    "\n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_article_keywords[i] = pd.to_numeric(df_article_keywords[i], errors='coerce')\n",
    "  \n",
    "# Drop NA Values in Keywords Column\n",
    "\n",
    "df_article_keywords['MaxLinkKeywords'].replace(' ', np.nan, inplace=True)\n",
    "df_article_keywords.dropna(subset=['MaxLinkKeywords'], inplace=True)\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_article_keywords[\"Type\"] = \"Article\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thumbnail Keyword DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list with only Thumbnail Keywords\n",
    "thumbnail_keywords = [\"Text\", \"MaxThumbnailKeywords\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "thumbnail_keywords.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with keywords and metrics\n",
    "df_thumbnail_keywords = df[thumbnail_keywords]\n",
    "\n",
    "\n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_thumbnail_keywords[i] = pd.to_numeric(df_thumbnail_keywords[i], errors='coerce')\n",
    "    \n",
    "# Drop NA Values in Keywords Column\n",
    "df_thumbnail_keywords['MaxThumbnailKeywords'].replace(' ', np.nan, inplace=True)\n",
    "df_thumbnail_keywords.dropna(subset=['MaxThumbnailKeywords'], inplace=True)\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_thumbnail_keywords[\"Type\"] = \"Thumbnails\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Keyword DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list with only Thumbnail Keywords\n",
    "post_keywords = [\"Text\", \"MaxTextKeywords\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "post_keywords.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with keywords and metrics\n",
    "df_post_keywords = df[post_keywords]\n",
    "\n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_post_keywords[i] = pd.to_numeric(df_post_keywords[i], errors='coerce')\n",
    "    \n",
    "# Drop NA Values in Keywords Column\n",
    "\n",
    "df_post_keywords['MaxTextKeywords'].replace(' ', np.nan, inplace=True)\n",
    "df_post_keywords.dropna(subset=['MaxTextKeywords'], inplace=True)\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_post_keywords[\"Type\"] = \"Posts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Post, Thumbnail, and Article DataFrames to Make One Keywords DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First make the column headers the same\n",
    "df_post_keywords.rename(columns={\"MaxTextKeywords\": \"Keywords\"}, inplace=True)\n",
    "\n",
    "df_thumbnail_keywords.rename(columns={\"MaxThumbnailKeywords\":\"Keywords\"}, inplace=True)\n",
    "\n",
    "df_article_keywords.rename(columns={\"MaxLinkKeywords\":\"Keywords\"}, inplace=True)\n",
    "\n",
    "# Combine into one data frame\n",
    "df_keywords = pd.concat([df_post_keywords, df_thumbnail_keywords, df_article_keywords])\n",
    "\n",
    "# Discard keywords with lower consumption to make charting easier\n",
    "df_keywords = df_keywords[df_keywords[\"Lifetime Post Consumptions\"] > 175]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='entity'></a>\n",
    "###  1.4 Create a Consolidated Entity DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Article Entity DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list with only Article Keywords\n",
    "article_entities = [\"Text\", \"MaxLinkEntity\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "article_entities.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with keywords and metrics\n",
    "df_article_entities = df[article_entities]\n",
    "    \n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_article_entities[i] = pd.to_numeric(df_article_entities[i], errors='coerce')\n",
    "    \n",
    "# Drop NA Values in Keywords Column\n",
    "df_article_entities['MaxLinkEntity'] = df[\"MaxLinkEntity\"].replace(r'\\s+', np.nan, regex=True)\n",
    "df_article_entities.dropna(subset=['MaxLinkEntity'], inplace=True)\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_article_entities[\"Type\"] = \"Article\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thumbnail Entity DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list with only Thumbnail Keywords\n",
    "thumbnail_entities = [\"Text\", \"MaxThumbnailEntity\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "thumbnail_entities.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with keywords and metrics\n",
    "df_thumbnail_entities = df[thumbnail_entities]\n",
    "\n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_thumbnail_entities[i] = pd.to_numeric(df_thumbnail_entities[i], errors='coerce')\n",
    "    \n",
    "# Drop NA Values in Keywords Column\n",
    "df_thumbnail_entities['MaxThumbnailEntity'] = df_thumbnail_entities['MaxThumbnailEntity'].replace(r'\\s+', np.nan, regex=True)\n",
    "df_thumbnail_entities.dropna(subset=['MaxThumbnailEntity'], inplace=True)\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_thumbnail_entities[\"Type\"] = \"Thumbnails\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Entity DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list with only Thumbnail Keywords\n",
    "post_entities = [\"Text\", \"MaxTextEntity\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "post_entities.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with keywords and metrics\n",
    "df_post_entities = df[post_entities]\n",
    "\n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_post_entities[i] = pd.to_numeric(df_post_entities[i], errors='coerce')\n",
    "    \n",
    "# Drop NA Values in Keywords Column\n",
    "df_post_entities['MaxTextEntity'] = df_post_entities['MaxTextEntity'].replace(r'\\s+', np.nan, regex=True)\n",
    "df_post_entities.dropna(subset=['MaxTextEntity'], inplace=True)\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_post_entities[\"Type\"] = \"Posts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Post, Thumbnail, and Article DataFrames to Make One Entity DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First make the column headers the same\n",
    "df_post_entities.rename(columns={\"MaxTextEntity\": \"Entities\"}, inplace=True)\n",
    "\n",
    "df_thumbnail_entities.rename(columns={\"MaxThumbnailEntity\":\"Entities\"}, inplace=True)\n",
    "\n",
    "df_article_entities.rename(columns={\"MaxLinkEntity\":\"Entities\"}, inplace=True)\n",
    "\n",
    "# Combine into one data frame\n",
    "df_entities = pd.concat([df_post_entities, df_thumbnail_entities, df_article_entities])\n",
    "\n",
    "df_entities[\"Entities\"] = df_entities[\"Entities\"].replace('', np.nan)\n",
    "df_entities.dropna(subset=[\"Entities\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='image_dataframe'></a>\n",
    "###  1.5 Create a Consolidated Image DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Metrics with Type Hierarchy, Class and Color to Make One Image DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list with only Visual Recognition columns\n",
    "pic_keywords = ['Image Type', 'Image Subtype', 'Image Subtype2', 'Image Class', 'Image Color']\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "pic_keywords.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with keywords and metrics\n",
    "df_pic_keywords = df[pic_keywords]\n",
    "\n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_pic_keywords[i] = pd.to_numeric(df_pic_keywords[i], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3\"></a> \n",
    "# Part III\n",
    "<a id='2setup'></a> \n",
    "## 1. Setup\n",
    "<a id='2setup2'></a>\n",
    "###  1.1 Assign Variables\n",
    "Assign new DataFrames to variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entities = df_entities\n",
    "tones = df_tones\n",
    "keywords = df_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2visual'></a>\n",
    "##  2. Visualize Data\n",
    "<a id='2visual1'></a> \n",
    "### 2.1 Run PixieDust Visualization Library with Display() API\n",
    "PixieDust lets you visualize your data in just a few clicks using the display() API. You can find more info at https://ibm-cds-labs.github.io/pixiedust/displayapi.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can use a pie chart to identify how post consumption was broken up by tone. \n",
    "\n",
    "Click on the `Options` button to change the chart.  Here are some things to try:\n",
    "* Add *Type* to make the breakdown show *Post* or *Article*.\n",
    "* Show *Social Tone* intead of *Emotion Tone* (or both).\n",
    "* Try a different metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "chartsize": "70",
      "handlerId": "pieChart",
      "keyFields": "Emotion Tone",
      "legend": "false",
      "mpld3": "false",
      "rowCount": "100",
      "title": "Tone in Posts and Articles",
      "valueFields": "Lifetime Post Consumptions"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(tones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at the same statistics as a bar chart.\n",
    "\n",
    "It is the same line of code. Use the `Edit Metadata` button to see how PixieDust knows to show us a bar chart. If you don't have a button use the menu and select `View > Cell Toolbar > Edit Metadata`.\n",
    "\n",
    "A bar chart is better at showing more information. We added `Cluster By: Type` so we already see numbers for posts and articles. Notice what the chart tells you. *Joy* seems to be the dominant emotion. Click on `Options` and try this:\n",
    "\n",
    "* Change the aggregation to `AVG`.\n",
    "\n",
    "What emotion leads to higher average consumption?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "chartsize": "70",
      "charttype": "grouped",
      "clusterby": "Type",
      "handlerId": "barChart",
      "keyFields": "Emotion Tone",
      "legend": "true",
      "rendererId": "matplotlib",
      "rowCount": "100",
      "title": "Tone in Posts and Articles",
      "valueFields": "Lifetime Post Consumptions"
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(tones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at the entities that were detected by Natural Language Understanding.\n",
    "\n",
    "The following bar chart shows the entities that were detected. This time we are stacking negative feedback and \"likes\" to get a picture of the kind of feedback the entities were getting. We chose a horizontal, stacked bar chart with descending values for a little variety.\n",
    "\n",
    "* Try a different renderer and see what you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "chartsize": "70",
      "charttype": "stacked",
      "handlerId": "barChart",
      "keyFields": "Entities",
      "orientation": "horizontal",
      "rendererId": "matplotlib",
      "rowCount": "100",
      "sortby": "Values ASC",
      "title": "Entities in Posts and Articles",
      "valueFields": "Lifetime Post Stories by action type - like,Lifetime Negative Feedback from Users"
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we look at the keywords detected by Natural Language Understanding\n",
    "\n",
    "This time we are using the bokeh renderer for a different look. The renderers have different capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "AVG",
      "chartsize": "99",
      "charttype": "stacked",
      "clusterby": "Type",
      "handlerId": "barChart",
      "keyFields": "Keywords",
      "legend": "true",
      "mpld3": "false",
      "orientation": "horizontal",
      "rendererId": "bokeh",
      "rowCount": "100",
      "sortby": "Values ASC",
      "timeseries": "false",
      "title": "Keywords in Posts, Articles and Thumbnails",
      "valueFields": "Lifetime Post Consumptions"
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's take a look at what Visual Recognition can show us.\n",
    "\n",
    "See how the images influenced the metrics. We've used visual recognition to identify a class and a type hierarchy for each image. We've also captured the top recognized color for each image. Our sample data doesn't have a significant number of data points, but these three charts demonstrate how you could:\n",
    "\n",
    "1. Recognize image classes that correlate to higher average post consumption.\n",
    "1. Add a type hierarchy for a higher level abstraction or to add grouping/stacking to the class data.\n",
    "1. Determine if image color correlates to user reactions.\n",
    "\n",
    "Visual recognition makes it surprisingly easy to do all of the above. Of course, you can easily try different metrics as you experiment. If you are not convinced that you should add ultramarine robot pictures to all of your articles, then you might want to do some research with a better data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "AVG",
      "chartsize": "99",
      "charttype": "stacked",
      "handlerId": "barChart",
      "keyFields": "Image Class",
      "legend": "false",
      "rowCount": "100",
      "sortby": "Values ASC",
      "title": "Image Class",
      "valueFields": "Lifetime Post Consumptions"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(df_pic_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "AVG",
      "chartsize": "99",
      "charttype": "stacked",
      "handlerId": "barChart",
      "keyFields": "Image Type,Image Subtype,Image Subtype2",
      "legend": "false",
      "rowCount": "100",
      "sortby": "Values ASC",
      "title": "Image Type Hierarchy",
      "valueFields": "Lifetime Post Consumptions"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(df_pic_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "AVG",
      "chartsize": "99",
      "charttype": "stacked",
      "handlerId": "barChart",
      "keyFields": "Image Color",
      "legend": "false",
      "rowCount": "100",
      "sortby": "Values ASC",
      "title": "Image Color",
      "valueFields": "Lifetime Post Consumptions"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(df_pic_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Info.\n",
    "For more information about PixieDust, check out the following:\n",
    "* PixieDust Documentation: https://ibm-cds-labs.github.io/pixiedust/index.html\n",
    "* PixieDust GitHub Repo: https://github.com/ibm-cds-labs/pixiedust\n",
    "\n",
    "Visit the Watson Accelerators portal to see more live patterns in action:\n",
    "* Watson Accelerators: http://www.watsonaccelerators.com "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3.5 (Experimental) with Spark 2.0",
   "language": "python",
   "name": "python3-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
