{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analyze Facebook Data Using IBM Watson and IBM Watson Studio\n",
    "\n",
    "This is a three-part notebook meant to show how anyone can enrich and analyze a combined dataset of unstructured and structured information with IBM Watson and IBM Watson Studio. For this example we are using a standard Facebook Analytics export which features texts from posts, articles and thumbnails, along with standard performance metrics such as likes, shares, and impressions. \n",
    "\n",
    "**Part I** will use the Natural Language Understanding and (optionally) Visual Recognition services from IBM Watson to enrich the Facebook posts, thumbnails, and articles by pulling out `Sentiment`, `Emotion`, `Entities`, `Keywords`, and `Images`. The end result of Part I will be additional features and metrics we can visualize in Part III. \n",
    "\n",
    "**Part II** will set up multiple pandas DataFrames that will contain the values, and metrics needed to find insights in the Part III tests and experiments.\n",
    "\n",
    "**Part III** will use charts to visualize the features that we discovered during enrichment and show how they correlate with customer impressions.\n",
    "\n",
    "\n",
    "#### You should only need to change data in the Setup portion of this notebook. All places where you see  <span style=\"color: red\"> User Input </span> is where you should be adding inputs. \n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "### [**Part I - Enrich**](#part1)<br>\n",
    "1. [Setup](#setup)<br>\n",
    "   1.1 [Install Watson Developer Cloud and BeautifulSoup Packages](#setup1)<br>\n",
    "   1.2 [Install PixieDust](#pixie)<br> \n",
    "   1.3 [Restart Kernel](#restart)<br> \n",
    "   1.4 [Import Packages and Libraries](#setup2)<br>\n",
    "   1.5 [Add Service Credentials From IBM Cloud for Watson Services](#setup3)<br>\n",
    "2. [Load Data](#load)<br>\n",
    "   2.1 [Load Data From Cloud Object Storage as a pandas DataFrame](#load1)<br>\n",
    "   2.2 [Set Variables](#load2)<br>\n",
    "3. [Prepare Data](#prepare)<br>\n",
    "   3.1 [Data Cleansing with Python](#prepare1)<br>\n",
    "   3.2 [Beautiful Soup to Extract Thumbnails and Extented Links](#prepare2)<br>\n",
    "4. [Enrich Data](#enrich)<br>\n",
    "   4.1 [NLU for Post Text](#nlupost)<br>\n",
    "   4.2 [NLU for Thumbnail Text](#nlutn)<br>\n",
    "   4.3 [NLU for Article Text](#nlulink)<br>\n",
    "   4.4 [Visual Recognition](#visual)<br>\n",
    "5. [Write Data](#write)<br>\n",
    "   5.1 [Convert DataFrame to new CSV](#write1)<br>\n",
    "   5.2 [Write Data to Cloud Object Storage](#write2)<br>\n",
    "    \n",
    "### [**Part II - Data Preparation**](#part2)<br>\n",
    "1. [Prepare Data](#prepare)<br>\n",
    "   1.1 [Create Multiple DataFrames for Visualizations](#visualizations)<br>\n",
    "   1.2 [Create a Consolidated Sentiment and Emotion DataFrame](#tone)<br>\n",
    "   1.3 [Create a Consolidated Keyword DataFrame](#keyword)<br>\n",
    "   1.4 [Create a Consolidated Entity DataFrame](#entity)<br>\n",
    "  \n",
    "### [**Part III - Analyze**](#part3)<br>\n",
    "\n",
    "1. [Setup](#2setup)<br> \n",
    "    1.1 [Assign Variables](#2setup2)<br>\n",
    "2. [Visualize Data](#2visual)<br>\n",
    "    2.1 [Run PixieDust Visualization Library with Display() API](#2visual1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"part1\"></a>Part I - Enrich\n",
    "\n",
    "## <a id=\"setup\"></a>1. Setup\n",
    "To prepare your environment, you need to install some packages and enter credentials for the Watson services.\n",
    "\n",
    "### <a id=\"setup1\"></a> 1.1 Install Latest Watson Developer Cloud and Beautiful Soup Packages\n",
    "You need to install these packages:\n",
    " - [Watson APIs Python SDK](https://github.com/watson-developer-cloud/python-sdk): a client library for Watson services.\n",
    " - <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\" target=\"_blank\" rel=\"noopener noreferrer\">Beautiful Soup</a>: a library to parse data from HTML for enriching the Facebook data.\n",
    " - <a href=\"https://ibm-cds-labs.github.io/pixiedust/\" target=\"_blank\" rel=\"noopener noreferrer\">PixieDust</a>: a library to visualize the data. \n",
    "\n",
    "Install the Watson Python SDK package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --user --no-warn-script-location ibm-watson==4.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Beautiful Soup package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --user beautifulsoup4==4.8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pixie\"></a>\n",
    "### 1.2 Install PixieDust Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --user --no-warn-script-location --upgrade pixiedust==1.1.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"restart\"></a>\n",
    "### 1.3 Restart Kernel\n",
    "> Required after installs/upgrades only.\n",
    "\n",
    "If any libraries were just installed or upgraded, <span style=\"color: red\">restart the kernel</span> before continuing. After this has been done once, you might want to comment out the `!pip install` lines above for cleaner output and a faster \"Run All\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup2\"></a>\n",
    "### 1.4 Import Packages and Libraries\n",
    "> Tip: To check if you have a package installed, open a new cell and write: `help(<package-name>)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from ibm_watson import VisualRecognitionV3\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from ibm_watson.natural_language_understanding_v1 import Features, EntitiesOptions, KeywordsOptions, EmotionOptions, SentimentOptions\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from operator import itemgetter\n",
    "from os.path import join, dirname\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# Suppress some pandas warnings\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# Suppress SSL warnings\n",
    "requests.packages.urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup3'></a>\n",
    "### 1.5 Add Service Credentials From IBM Cloud for Watson Services\n",
    "Edit the following cell to provide your credentials for Watson and Natural Language Understanding and Visual Recognition.\n",
    "\n",
    "You must create a Watson Natural Language Understanding service and, optionally, a Watson Visual Recognition service on [IBM Cloud](https://cloud.ibm.com/).\n",
    "\n",
    "1. Create a service for [Natural Language Understanding (NLU)](https://cloud.ibm.com/catalog/services/natural-language-understanding). \n",
    "1. Create a service for [Visual Recognition](https://cloud.ibm.com/catalog/services/visual-recognition).\n",
    "1. Insert API keys and URLs in the following cell.\n",
    "1. Run the cell.\n",
    "\n",
    "###  <span style=\"color: red\"> _User Input_</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "\n",
    "# Watson Natural Language Understanding (NLU)\n",
    "NATURAL_LANGUAGE_UNDERSTANDING_API_KEY = ''\n",
    "NATURAL_LANGUAGE_UNDERSTANDING_URL = ''\n",
    "\n",
    "# Watson Visual Recognition (optional)\n",
    "VISUAL_RECOGNITION_API_KEY = ''\n",
    "VISUAL_RECOGNITION_URL = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Watson clients\n",
    "\n",
    "nlu_auth = IAMAuthenticator(NATURAL_LANGUAGE_UNDERSTANDING_API_KEY)\n",
    "nlu = NaturalLanguageUnderstandingV1(version='2020-03-09',\n",
    "                                     authenticator=nlu_auth)\n",
    "nlu.set_service_url(NATURAL_LANGUAGE_UNDERSTANDING_URL)\n",
    "\n",
    "visual_recognition = False  # Making visrec optional.\n",
    "if VISUAL_RECOGNITION_API_KEY and VISUAL_RECOGNITION_URL:\n",
    "    vr_auth = IAMAuthenticator(VISUAL_RECOGNITION_API_KEY)\n",
    "    visual_recognition = VisualRecognitionV3(version='2019-03-09',\n",
    "                                             authenticator=vr_auth)\n",
    "    visual_recognition.set_service_url(VISUAL_RECOGNITION_URL)\n",
    "else:\n",
    "    print(\"Skipping Visual Recognition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='load'></a>2. Load Data\n",
    "The data you'll analyzing is a sample of a standard export of the Facebook Insights Post information from the <a href=\"https://www.facebook.com/ibmwatson/\" target=\"_blank\" rel=\"noopener noreferrer\">IBM Watson Facebook page</a>. Engagement metrics such as clicks, impressions, and so on, are altered and do not reflect actual post performance data. The data is on the Watson Studio community page.\n",
    "\n",
    "### 2.1 Load the data as a pandas DataFrame\n",
    "\n",
    "To get the data and load it into a pandas DataFrame:\n",
    "\n",
    "1. Load the file by clicking the **Find and Add Data** icon and then dragging and dropping the file onto the pane or browsing for the file. The data is stored in the object storage container that is associated with your project.\n",
    "1. Click in the next cell and then choose **Insert to code > pandas DataFrame** from below the file name and then run the cell. Change the inserted variable name to `df_data_1`\n",
    "\n",
    "###  <span style=\"color: red\"> _User Input_</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Insert to code > pandas DataFrame**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='load2'></a>2.2 Set variables\n",
    "You need to set these variables:\n",
    " - The name of the DataFrame\n",
    " - Your credentials for the source file\n",
    " - A file name for the enriched DataFrame\n",
    " \n",
    "Define a variable, `df`, for the DataFrame that you just created. If necessary, change the original DataFrame name to match the one you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure this uses the variable above. The number will vary in the inserted code.\n",
    "try:\n",
    "    df = df_data_1\n",
    "except NameError as e:\n",
    "    print('Error: Setup is incorrect or incomplete.\\n')\n",
    "    print('Follow the instructions to insert the pandas DataFrame above, and edit to')\n",
    "    print('make the generated df_data_# variable match the variable used here.')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the cell below and place your cursor on an empty line below the comment.** \n",
    "Put in the credentials for the file you want to enrich by clicking on the 10/01 (upper right), then click `Insert to code` under the file you want to enrich, and choose `Insert Credentials`.\n",
    "\n",
    "**Change the inserted variable name to `credentials_1`**\n",
    "\n",
    "###  <span style=\"color: red\"> _User Input_</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert credentials for file - Change to credentials_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure this uses the variable above. The number will vary in the inserted code.\n",
    "try:\n",
    "    credentials = credentials_1\n",
    "except NameError as e:\n",
    "    print('Error: Setup is incorrect or incomplete.\\n')\n",
    "    print('Follow the instructions to insert the file credentials above, and edit to')\n",
    "    print('make the generated credentials_# variable match the variable used here.')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prepare'></a>\n",
    "## 3. Prepare Data\n",
    "You'll prepare the data by cleansing it and extracting the URLs. Many of the posts contain both text and a URL. The first task is to separate URLs from the text so that they can be analyzed separately. Then you need to get thumbnails for the photos and links, and convert any shortened URLs to full URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prepare1'></a>\n",
    "###  3.1 Data Cleansing with Python\n",
    "Renaming columns, removing noticeable noise in the data, pulling out URLs and appending to a new column to run through NLU.\n",
    "\n",
    "To cleanse the data, you'll rename a column and add a column with the URLs that were embedded in the post. \n",
    "\n",
    "Change the name of the `Post Message` column to `Text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Post Message': 'Text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows that have NaN for the text.\n",
    "df.dropna(subset=['Text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `str.partition` function to remove strings that contain \"http\" and \"www\" from the `Text` column and save them in new DataFrames, then add all web addresses to a new `Link` column in the original DataFrame. This process captures all web addresses: https, http, and www."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_http = df[\"Text\"].str.partition(\"http\")\n",
    "df_www = df[\"Text\"].str.partition(\"www\")\n",
    "\n",
    "# Combine delimiters with actual links\n",
    "df_http[\"Link\"] = df_http[1].map(str) + df_http[2]\n",
    "df_www[\"Link1\"] = df_www[1].map(str) + df_www[2]\n",
    "\n",
    "# Include only Link columns\n",
    "df_http.drop(df_http.columns[0:3], axis=1, inplace = True)\n",
    "df_www.drop(df_www.columns[0:3], axis=1, inplace = True)\n",
    "\n",
    "# Merge http and www DataFrames\n",
    "dfmerge = pd.concat([df_http, df_www], axis=1)\n",
    "\n",
    "# The following steps will allow you to merge data columns from the left to the right\n",
    "dfmerge = dfmerge.apply(lambda x: x.str.strip()).replace('', np.nan)\n",
    "\n",
    "# Use fillna to fill any blanks with the Link1 column\n",
    "dfmerge[\"Link\"].fillna(dfmerge[\"Link1\"], inplace = True)\n",
    "\n",
    "# Delete Link1 (www column)\n",
    "dfmerge.drop(\"Link1\", axis=1, inplace = True)\n",
    "\n",
    "# Combine Link data frame\n",
    "df = pd.concat([dfmerge,df], axis = 1)\n",
    "\n",
    "# Make sure text column is a string\n",
    "df[\"Text\"] = df[\"Text\"].astype(\"str\")\n",
    "\n",
    "# Strip links from Text column\n",
    "df['Text'] = df['Text'].apply(lambda x: x.split('http')[0])\n",
    "df['Text'] = df['Text'].apply(lambda x: x.split('www')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='prepare2'></a> 3.2 Extract thumbnails and extended links\n",
    "\n",
    "A standard Facebook export does not provide the thumbnail that usually summarizes the link or photo associated with each post. Use the Beautiful Soup library to go into the HTML of the post and extract the thumbnail text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change links from objects to strings\n",
    "for link in df.Link:\n",
    "    df.Link.to_string()\n",
    "\n",
    "piclinks = []\n",
    "description = []\n",
    "for url in df[\"Link\"]:\n",
    "    if pd.isnull(url):\n",
    "        piclinks.append(\"\")\n",
    "        description.append(\"\")\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        # Skip certificate check with verify=False.\n",
    "        # Don't do this if your urls are not secure.\n",
    "        page3 = requests.get(url, verify=False)\n",
    "        if page3.status_code != requests.codes.ok:\n",
    "            piclinks.append(\"\")\n",
    "            description.append(\"\")\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(\"Skipping url %s: %s\" % (url, e))\n",
    "        piclinks.append(\"\")\n",
    "        description.append(\"\")\n",
    "        continue\n",
    "        \n",
    "    soup3 = bs(page3.text,\"lxml\")\n",
    "    \n",
    "    pic = soup3.find('meta', property =\"og:image\")\n",
    "    if pic:\n",
    "        piclinks.append(pic[\"content\"])\n",
    "    else: \n",
    "        piclinks.append(\"\")\n",
    "    \n",
    "    content = None\n",
    "    desc = soup3.find(attrs={'name':'Description'})\n",
    "    if desc:\n",
    "        content = desc['content']\n",
    "    if not content or content == 'null':\n",
    "        # Try again with lowercase description\n",
    "        desc = soup3.find(attrs={'name':'description'})\n",
    "        if desc:\n",
    "            content = desc['content']\n",
    "    if not content or content == 'null':\n",
    "        description.append(\"\")\n",
    "    else:\n",
    "        description.append(content)\n",
    "            \n",
    "# Save thumbnail descriptions to df in a column titled 'Thumbnails'\n",
    "df[\"Thumbnails\"] = description\n",
    "# Save image links to df in a column titled 'Image'\n",
    "df[\"Image\"] = piclinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert shortened links to full links.\n",
    "Use requests module to pull extended links. This is only necessary if the Facebook page uses different links than the articles themselves. For this example we are using IBM Watson's Facebook export which uses an IBM link. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortlink = df[\"Link\"]\n",
    "extendedlink = []\n",
    "\n",
    "for link in shortlink:\n",
    "    if isinstance(link, float):  # Float is not a URL, probably NaN.\n",
    "        extendedlink.append('')\n",
    "    else:\n",
    "        try:\n",
    "            extended_link = requests.Session().head(link, allow_redirects=True).url\n",
    "            extendedlink.append(extended_link)\n",
    "        except Exception as e:\n",
    "            print(\"Skipping link %s: %s\" % (link, e))\n",
    "            extendedlink.append('')\n",
    "\n",
    "df[\"Extended Links\"] = extendedlink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='enrich'></a> \n",
    "## 4. Enrichment Time!\n",
    "<a id='nlupost'></a>\n",
    "###  4.1 NLU for the Post Text\n",
    "The following script is an example of how to use Natural Language Understanding to iterate through each post and extract enrichment features for future analysis.\n",
    "\n",
    "For this example, we are looking at the `Text` column in our DataFrame, which contains the text of each post. NLU can also iterate through a column of URLs, or other freeform text. There's a list within a list for the Keywords and Entities features to allow gathering multiple entities and keywords from each piece of text.\n",
    "\n",
    "Each extracted feature is appended to the DataFrame in a new column that's defined at the end of the script. If you want to run this same script for the other columns, set the loop iterable to the column name, if you are using URLs, change the `text=response` parameter to `url=response`, and update the new column names as necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of features to get enrichment values for entities, keywords, emotion and sentiment\n",
    "features = Features(entities=EntitiesOptions(), keywords=KeywordsOptions(), emotion=EmotionOptions(), sentiment=SentimentOptions())\n",
    "\n",
    "overallSentimentScore = []\n",
    "overallSentimentType = []\n",
    "highestEmotion = []\n",
    "highestEmotionScore = []\n",
    "kywords = []\n",
    "entities = []\n",
    "\n",
    "# Go through every response and enrich the text using NLU.\n",
    "for text in df['Text']:\n",
    "  if not text:\n",
    "    # print(\"Text is empty\")\n",
    "    overallSentimentScore.append('0')\n",
    "    overallSentimentType.append('0')\n",
    "    highestEmotion.append(\"\")\n",
    "    highestEmotionScore.append(\"\")\n",
    "    kywords.append(\"\")\n",
    "    entities.append(\"\")\n",
    "    continue\n",
    "  else:\n",
    "    # We are assuming English to avoid errors when the language cannot be detected.\n",
    "    enriched_json = nlu.analyze(text=text, features=features, language='en').get_result()\n",
    "\n",
    "    # Get the SENTIMENT score and type\n",
    "    if 'sentiment' in enriched_json:\n",
    "        if('score' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentScore.append(enriched_json[\"sentiment\"][\"document\"][\"score\"])\n",
    "        else:\n",
    "            overallSentimentScore.append('0')\n",
    "\n",
    "        if('label' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentType.append(enriched_json[\"sentiment\"][\"document\"][\"label\"])\n",
    "        else:\n",
    "            overallSentimentType.append('0')\n",
    "    else:\n",
    "        overallSentimentScore.append('0')\n",
    "        overallSentimentType.append('0')\n",
    "\n",
    "    # Read the EMOTIONS into a dict and get the key (emotion) with maximum value\n",
    "    if 'emotion' in enriched_json:\n",
    "        me = max(enriched_json[\"emotion\"][\"document\"][\"emotion\"].items(), key=operator.itemgetter(1))[0]\n",
    "        highestEmotion.append(me)\n",
    "        highestEmotionScore.append(enriched_json[\"emotion\"][\"document\"][\"emotion\"][me])\n",
    "    else:\n",
    "        highestEmotion.append(\"\")\n",
    "        highestEmotionScore.append(\"\")\n",
    "\n",
    "    # Iterate and get KEYWORDS with a confidence of over 70%\n",
    "    if 'keywords' in enriched_json:\n",
    "        tmpkw = []\n",
    "        for kw in enriched_json['keywords']:\n",
    "            if(float(kw[\"relevance\"]) >= 0.7):\n",
    "                tmpkw.append(kw[\"text\"])\n",
    "        # Convert multiple keywords in a list to a string and append the string\n",
    "        kywords.append(', '.join(tmpkw))\n",
    "    else:\n",
    "        kywords.append(\"\")\n",
    "            \n",
    "    # Iterate and get Entities with a confidence of over 30%\n",
    "    if 'entities' in enriched_json:\n",
    "        tmpent = []\n",
    "        for ent in enriched_json['entities']: \n",
    "            if(float(ent[\"relevance\"]) >= 0.3):\n",
    "                tmpent.append(ent[\"type\"])\n",
    " \n",
    "        # Convert multiple entities in a list to a string and append the string\n",
    "        entities.append(', '.join(tmpent))\n",
    "    else:\n",
    "        entities.append(\"\")\n",
    "    \n",
    "# Create columns from the list and append to the DataFrame\n",
    "if highestEmotion:\n",
    "    df['TextHighestEmotion'] = highestEmotion\n",
    "if highestEmotionScore:\n",
    "    df['TextHighestEmotionScore'] = highestEmotionScore\n",
    "\n",
    "if overallSentimentType:\n",
    "    df['TextOverallSentimentType'] = overallSentimentType\n",
    "if overallSentimentScore:\n",
    "    df['TextOverallSentimentScore'] = overallSentimentScore\n",
    "\n",
    "df['TextKeywords'] = kywords\n",
    "df['TextEntities'] = entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we extract all of the Keywords and Entities from each Post, we have columns with multiple Keywords and Entities separated by commas. For our Analysis in Part II, we also wanted the top Keyword and Entity for each Post. Because of this, we added two new columns to capture the `MaxTextKeyword` and `MaxTextEntity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose first of Keywords and Entities\n",
    "df[\"MaxTextKeywords\"] = df[\"TextKeywords\"].apply(lambda x: x.split(',')[0])\n",
    "df[\"MaxTextEntity\"] = df[\"TextEntities\"].apply(lambda x: x.split(',')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nlutn'></a>\n",
    "###  4.2 NLU for Thumbnail Text\n",
    "\n",
    "We will repeat the same process for Thumbnails and Article Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of features to get enrichment values for entities, keywords, emotion and sentiment\n",
    "features = Features(entities=EntitiesOptions(), keywords=KeywordsOptions(), emotion=EmotionOptions(), sentiment=SentimentOptions())\n",
    "\n",
    "overallSentimentScore = []\n",
    "overallSentimentType = []\n",
    "highestEmotion = []\n",
    "highestEmotionScore = []\n",
    "kywords = []\n",
    "entities = []\n",
    "\n",
    "# Go through every response and enrich the text using NLU.\n",
    "for text in df['Thumbnails']:\n",
    "    if not text:\n",
    "        overallSentimentScore.append(' ')\n",
    "        overallSentimentType.append(' ')\n",
    "        highestEmotion.append(' ')\n",
    "        highestEmotionScore.append(' ')\n",
    "        kywords.append(' ')\n",
    "        entities.append(' ')\n",
    "        continue\n",
    "\n",
    "    enriched_json = nlu.analyze(text=text, features=features, language='en').get_result()\n",
    "\n",
    "    # Get the SENTIMENT score and type\n",
    "    if 'sentiment' in enriched_json:\n",
    "        if('score' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentScore.append(enriched_json[\"sentiment\"][\"document\"][\"score\"])\n",
    "        else:\n",
    "            overallSentimentScore.append(\"\")\n",
    "\n",
    "        if('label' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentType.append(enriched_json[\"sentiment\"][\"document\"][\"label\"])\n",
    "        else:\n",
    "            overallSentimentType.append(\"\")\n",
    "\n",
    "    # Read the EMOTIONS into a dict and get the key (emotion) with maximum value\n",
    "    if 'emotion' in enriched_json:\n",
    "        me = max(enriched_json[\"emotion\"][\"document\"][\"emotion\"].items(), key=operator.itemgetter(1))[0]\n",
    "        highestEmotion.append(me)\n",
    "        highestEmotionScore.append(enriched_json[\"emotion\"][\"document\"][\"emotion\"][me])\n",
    "\n",
    "    else:\n",
    "        highestEmotion.append(\"\")\n",
    "        highestEmotionScore.append(\"\")\n",
    "\n",
    "    # Iterate and get KEYWORDS with a confidence of over 70%\n",
    "    if 'keywords' in enriched_json:\n",
    "        tmpkw = []\n",
    "        for kw in enriched_json['keywords']:\n",
    "            if(float(kw[\"relevance\"]) >= 0.7):\n",
    "                tmpkw.append(kw[\"text\"])\n",
    "        # Convert multiple keywords in a list to a string and append the string\n",
    "        kywords.append(', '.join(tmpkw))\n",
    "     \n",
    "    # Iterate and get Entities with a confidence of over 30%\n",
    "    if 'entities' in enriched_json:\n",
    "        tmpent = []\n",
    "        for ent in enriched_json['entities']:              \n",
    "            if(float(ent[\"relevance\"]) >= 0.3):\n",
    "                tmpent.append(ent[\"type\"])\n",
    "        # Convert multiple entities in a list to a string and append the string\n",
    "        entities.append(', '.join(tmpent))\n",
    "    else:\n",
    "        entities.append(\"\")     \n",
    "  \n",
    "# Create columns from the list and append to the DataFrame\n",
    "if highestEmotion:\n",
    "    df['ThumbnailHighestEmotion'] = highestEmotion\n",
    "if highestEmotionScore:\n",
    "    df['ThumbnailHighestEmotionScore'] = highestEmotionScore\n",
    "\n",
    "if overallSentimentType:\n",
    "    df['ThumbnailOverallSentimentType'] = overallSentimentType\n",
    "if overallSentimentScore:\n",
    "    df['ThumbnailOverallSentimentScore'] = overallSentimentScore\n",
    "\n",
    "df['ThumbnailKeywords'] = kywords\n",
    "df['ThumbnailEntities'] = entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Add two new columns to capture the `MaxThumbnailKeyword` and `MaxThumbnailEntity`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'Max' to first one from keywords and entities lists\n",
    "df[\"MaxThumbnailKeywords\"] = df[\"ThumbnailKeywords\"].apply(lambda x: x.split(',')[0])\n",
    "df[\"MaxThumbnailEntity\"] = df[\"ThumbnailEntities\"].apply(lambda x: x.split(',')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nlulink'></a> \n",
    "### 4.3 NLU for Article Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of features to get enrichment values for entities, keywords, emotion and sentiment\n",
    "features = Features(entities=EntitiesOptions(), keywords=KeywordsOptions(), emotion=EmotionOptions(), sentiment=SentimentOptions())\n",
    "\n",
    "overallSentimentScore = []\n",
    "overallSentimentType = []\n",
    "highestEmotion = []\n",
    "highestEmotionScore = []\n",
    "kywords = []\n",
    "entities = []\n",
    "article_text = []\n",
    "        \n",
    "# Go through every response and enrich the article using NLU\n",
    "for url in df['Extended Links']:\n",
    "    if not url:\n",
    "        overallSentimentScore.append(' ')\n",
    "        overallSentimentType.append(' ')\n",
    "        highestEmotion.append(' ')\n",
    "        highestEmotionScore.append(' ')\n",
    "        kywords.append(' ')\n",
    "        entities.append(' ')\n",
    "        article_text.append(' ')\n",
    "        continue\n",
    "\n",
    "    # Run links through NLU to get entities, keywords, emotion and sentiment.\n",
    "    # Use return_analyzed_text to extract text for Tone Analyzer to use.\n",
    "    \n",
    "    try:\n",
    "        enriched_json = nlu.analyze(url=url,\n",
    "                                features=features,\n",
    "                                language='en',\n",
    "                                return_analyzed_text=True).get_result()\n",
    "        article_text.append(enriched_json[\"analyzed_text\"])\n",
    "    except Exception as e:\n",
    "        print(\"Skipping url %s: %s\" % (url, e))\n",
    "        overallSentimentScore.append(' ')\n",
    "        overallSentimentType.append(' ')\n",
    "        highestEmotion.append(' ')\n",
    "        highestEmotionScore.append(' ')\n",
    "        kywords.append(' ')\n",
    "        entities.append(' ')\n",
    "        article_text.append(' ')\n",
    "        continue\n",
    "      \n",
    "    \n",
    "    \n",
    "\n",
    "    # Get the SENTIMENT score and type\n",
    "    if 'sentiment' in enriched_json:\n",
    "        if('score' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentScore.append(enriched_json[\"sentiment\"][\"document\"][\"score\"])\n",
    "        else:\n",
    "            overallSentimentScore.append('None')\n",
    "\n",
    "        if('label' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentType.append(enriched_json[\"sentiment\"][\"document\"][\"label\"])\n",
    "        else:\n",
    "            overallSentimentType.append('')\n",
    "\n",
    "    # Read the EMOTIONS into a dict and get the key (emotion) with maximum value\n",
    "    if 'emotion' in enriched_json:\n",
    "        me = max(enriched_json[\"emotion\"][\"document\"][\"emotion\"].items(), key=operator.itemgetter(1))[0]\n",
    "        highestEmotion.append(me)\n",
    "        highestEmotionScore.append(enriched_json[\"emotion\"][\"document\"][\"emotion\"][me])\n",
    "\n",
    "    else:\n",
    "        highestEmotion.append('')\n",
    "        highestEmotionScore.append('')\n",
    "\n",
    "    # Iterate and get KEYWORDS with a confidence of over 70%\n",
    "    if 'keywords' in enriched_json:\n",
    "        tmpkw = []\n",
    "        for kw in enriched_json['keywords']:\n",
    "            if(float(kw[\"relevance\"]) >= 0.7):\n",
    "                tmpkw.append(kw[\"text\"])\n",
    "        # Convert multiple keywords in a list to a string and append the string\n",
    "        kywords.append(', '.join(tmpkw))\n",
    "    else: \n",
    "        kywords.append(\"\")\n",
    "            \n",
    "    # Iterate and get Entities with a confidence of over 30%\n",
    "    if 'entities' in enriched_json:\n",
    "        tmpent = []\n",
    "        for ent in enriched_json['entities']:               \n",
    "            if(float(ent[\"relevance\"]) >= 0.3):\n",
    "                tmpent.append(ent[\"type\"])\n",
    "        # Convert multiple entities in a list to a string and append the string\n",
    "        entities.append(', '.join(tmpent))\n",
    "    else:\n",
    "        entities.append(\"\")\n",
    "    \n",
    "# Create columns from the list and append to the DataFrame\n",
    "if highestEmotion:\n",
    "    df['LinkHighestEmotion'] = highestEmotion\n",
    "if highestEmotionScore:\n",
    "    df['LinkHighestEmotionScore'] = highestEmotionScore\n",
    "\n",
    "if overallSentimentType:\n",
    "    df['LinkOverallSentimentType'] = overallSentimentType\n",
    "if overallSentimentScore:\n",
    "    df['LinkOverallSentimentScore'] = overallSentimentScore\n",
    "\n",
    "df['LinkKeywords'] = kywords\n",
    "df['LinkEntities'] = entities\n",
    "df['Article Text'] = article_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add two new columns to capture the `MaxLinkKeyword` and `MaxLinkEntity`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'Max' to first one from keywords and entities lists\n",
    "df[\"MaxLinkKeywords\"] = df[\"LinkKeywords\"].apply(lambda x: x.split(',')[0])\n",
    "df[\"MaxLinkEntity\"] = df[\"LinkEntities\"].apply(lambda x: x.split(',')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visual'></a> \n",
    "### 4.4 Visual Recognition\n",
    "Below uses Visual Recognition to classify the thumbnail images.\n",
    "\n",
    "> NOTE: When using the **free tier** of Visual Recognition, _classify_ has a limit of 250 images per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visual_recognition:\n",
    "    piclinks = df[\"Image\"]\n",
    "\n",
    "    picclass = []\n",
    "    piccolor = []\n",
    "    pictype1 = []\n",
    "    pictype2 = []\n",
    "    pictype3 = []\n",
    "\n",
    "    for pic in piclinks:\n",
    "        if not pic or pic == 'default-img':\n",
    "            picclass.append(' ')\n",
    "            piccolor.append(' ')\n",
    "            pictype1.append(' ')\n",
    "            pictype2.append(' ')\n",
    "            pictype3.append(' ')\n",
    "            continue\n",
    "\n",
    "        classes = []\n",
    "        enriched_json = {}\n",
    "        try:\n",
    "            enriched_json = visual_recognition.classify(url=pic).get_result()\n",
    "        except Exception as e:\n",
    "            print(\"Skipping url %s: %s\" % (pic, e))\n",
    "\n",
    "        if 'error' in enriched_json:\n",
    "            print(enriched_json['error'])\n",
    "        if 'images' in enriched_json and 'classifiers' in enriched_json['images'][0]:\n",
    "            classes = enriched_json['images'][0][\"classifiers\"][0][\"classes\"]\n",
    "\n",
    "        color1 = None\n",
    "        class1 = None\n",
    "        type_hierarchy1 = None\n",
    "\n",
    "        for iclass in classes:\n",
    "            # Grab the first color, first class, and first type hierarchy.\n",
    "            # Note: Usually you'd filter by 'score' too.\n",
    "            if not type_hierarchy1 and 'type_hierarchy' in iclass:\n",
    "                type_hierarchy1 = iclass['type_hierarchy']\n",
    "            if not class1:\n",
    "                class1 = iclass['class']\n",
    "            if not color1 and iclass['class'].endswith(' color'):\n",
    "                color1 = iclass['class'][:-len(' color')]\n",
    "            if type_hierarchy1 and class1 and color1:\n",
    "                # We are only using 1 of each per image. When we have all 3, break.\n",
    "                break\n",
    "\n",
    "        picclass.append(class1 or ' ')\n",
    "        piccolor.append(color1 or ' ')\n",
    "        type_split = (type_hierarchy1 or '/ / / ').split('/')\n",
    "        pictype1.append(type_split[1] if len(type_split) > 1 else '-')\n",
    "        pictype2.append(type_split[2] if len(type_split) > 2 else '- ')\n",
    "        pictype3.append(type_split[3] if len(type_split) > 3 else '-')\n",
    "\n",
    "    df[\"Image Color\"] = piccolor\n",
    "    df[\"Image Class\"] = picclass\n",
    "    df[\"Image Type\"] = pictype1\n",
    "    df[\"Image Subtype\"] = pictype2\n",
    "    df[\"Image Subtype2\"] = pictype3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='write'></a>\n",
    "## Enrichment is now COMPLETE!\n",
    "<a id='write1'></a> \n",
    "Save a copy of the enriched DataFrame as a file in Cloud Object Storage. To run the upload_file function we first need to create a variable that contains our credentials we created in section 2.2. No user input is required as we already have all of the information we need. To upload the file to COS simply run the next two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n",
    "    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n",
    "    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url=credentials['ENDPOINT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the enriched file name from the original filename.\n",
    "localfilename = 'enriched_' + credentials['FILE']\n",
    "\n",
    "# Write a CSV file from the enriched pandas DataFrame.\n",
    "df.to_csv(localfilename, index=False)\n",
    "\n",
    "# Use the above put_file method with credentials to put the file in Object Storage.\n",
    "cos.upload_file(localfilename, Bucket=credentials['BUCKET'],Key=localfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to use the enriched local file, you can read it back in.\n",
    "# This might be handy if you already enriched and just want to re-run\n",
    "# from this cell and below. Uncomment the following line.\n",
    "\n",
    "# df = pd.read_csv(localfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a> \n",
    "# Part II - Data Preparation\n",
    "<a id='prepare'></a>\n",
    "## 1. Prepare Data\n",
    " <a id='visualizations'></a>\n",
    "### 1.1 Prepare Multiple DataFrames for Visualizations\n",
    "Before we can create the separate tables for each Watson feature we need to organize and reformat the data. First, we need to determine which data points are tied to metrics. Second, we need to make sure make sure each metric is numeric. _(This is necessary for PixieDust in Part III)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the lifetime metrics in a list\n",
    "metrics = [metric for metric in df.columns.values.tolist() if 'Lifetime' in metric]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tone'></a> \n",
    "### 1.2 Create a Consolidated Sentiment and Emotion DataFrame\n",
    "You'll create a DataFrame for the sentiment and emotion of the post text and a DataFrame for the sentiment and emotion of the article text. Then you'll combine them into one DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Sentiment and Emotion DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with only Post sentiment and emotion values\n",
    "post_tones = [\"Text\",\"TextHighestEmotion\", \"TextHighestEmotionScore\", \"TextOverallSentimentType\", \"TextOverallSentimentScore\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "post_tones.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with metrics and sentiment and emotion\n",
    "df_post_tones = df[post_tones]\n",
    "\n",
    "# Determine which tone values are suppose to be numeric and ensure they are numeric. \n",
    "post_numeric_values = [\"TextHighestEmotionScore\", \"TextOverallSentimentScore\"]\n",
    "for i in post_numeric_values:\n",
    "    df_post_tones[i] = pd.to_numeric(df_post_tones[i], errors='coerce')\n",
    "\n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_post_tones[i] = pd.to_numeric(df_post_tones[i], errors='coerce')\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_post_tones[\"Type\"] = \"Post\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Article Sentiment and Emotion DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with only Article sentiment and emotion values\n",
    "article_tones = [\"Text\", \"LinkHighestEmotion\", \"LinkHighestEmotionScore\", \"LinkOverallSentimentType\", \"LinkOverallSentimentScore\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "article_tones.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with metrics and sentiment and emotion\n",
    "df_article_tones = df[article_tones]\n",
    "\n",
    "# Determine which values are suppose to be numeric and ensure they are numeric. \n",
    "art_numeric_values = [\"LinkHighestEmotionScore\", \"LinkOverallSentimentScore\"]\n",
    "for i in art_numeric_values:\n",
    "    df_article_tones[i] = pd.to_numeric(df_article_tones[i], errors='coerce')\n",
    "    \n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_article_tones[i] = pd.to_numeric(df_article_tones[i], errors='coerce')\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_article_tones[\"Type\"] = \"Article\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Post and Article DataFrames to Make DataFrame with Sentiment and Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make the Column Headers the same\n",
    "df_post_tones.rename(columns={\"TextHighestEmotion\":\"Emotion\",\n",
    "                              \"TextHighestEmotionScore\":\"Emotion Score\",\n",
    "                              \"TextOverallSentimentType\": \"Sentiment\",\n",
    "                              \"TextOverallSentimentScore\": \"Sentiment Score\"\n",
    "                             },\n",
    "                     inplace=True)\n",
    "\n",
    "df_article_tones.rename(columns={\"LinkHighestEmotion\":\"Emotion\",\n",
    "                                 \"LinkHighestEmotionScore\":\"Emotion Score\",\n",
    "                                 \"LinkOverallSentimentType\": \"Sentiment\",\n",
    "                                 \"LinkOverallSentimentScore\": \"Sentiment Score\"\n",
    "                                },\n",
    "                        inplace=True)\n",
    "\n",
    "# Combine into one data frame\n",
    "df_tones = pd.concat([df_post_tones, df_article_tones])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='keyword'></a> \n",
    "### 1.3 Create a Consolidated Keyword DataFrame\n",
    "You'll create DataFrames for the keywords of the article text, the thumbnail text, and the post text. Then you'll combine them into one DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Article Keyword DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with only Article Keywords\n",
    "article_keywords = [\"Text\", \"MaxLinkKeywords\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "article_keywords.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with keywords and metrics\n",
    "df_article_keywords = df[article_keywords]\n",
    "\n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_article_keywords[i] = pd.to_numeric(df_article_keywords[i], errors='coerce')\n",
    "\n",
    "# Drop NA Values in Keywords Column\n",
    "df_article_keywords['MaxLinkKeywords'].replace(' ', np.nan, inplace=True)\n",
    "df_article_keywords.dropna(subset=['MaxLinkKeywords'], inplace=True)\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_article_keywords[\"Type\"] = \"Article\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thumbnail Keyword DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with only Thumbnail Keywords\n",
    "thumbnail_keywords = [\"Text\", \"MaxThumbnailKeywords\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "thumbnail_keywords.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with keywords and metrics\n",
    "df_thumbnail_keywords = df[thumbnail_keywords]\n",
    "\n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_thumbnail_keywords[i] = pd.to_numeric(df_thumbnail_keywords[i], errors='coerce')\n",
    "    \n",
    "# Drop NA Values in Keywords Column\n",
    "df_thumbnail_keywords['MaxThumbnailKeywords'].replace(' ', np.nan, inplace=True)\n",
    "df_thumbnail_keywords.dropna(subset=['MaxThumbnailKeywords'], inplace=True)\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_thumbnail_keywords[\"Type\"] = \"Thumbnails\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Keyword DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with only Thumbnail Keywords\n",
    "post_keywords = [\"Text\", \"MaxTextKeywords\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "post_keywords.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with keywords and metrics\n",
    "df_post_keywords = df[post_keywords]\n",
    "\n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_post_keywords[i] = pd.to_numeric(df_post_keywords[i], errors='coerce')\n",
    "    \n",
    "# Drop NA Values in Keywords Column\n",
    "df_post_keywords['MaxTextKeywords'].replace(' ', np.nan, inplace=True)\n",
    "df_post_keywords.dropna(subset=['MaxTextKeywords'], inplace=True)\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_post_keywords[\"Type\"] = \"Posts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Post, Thumbnail, and Article DataFrames to Make One Keywords DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make the column headers the same\n",
    "df_post_keywords.rename(columns={\"MaxTextKeywords\": \"Keywords\"}, inplace=True)\n",
    "df_thumbnail_keywords.rename(columns={\"MaxThumbnailKeywords\":\"Keywords\"}, inplace=True)\n",
    "df_article_keywords.rename(columns={\"MaxLinkKeywords\":\"Keywords\"}, inplace=True)\n",
    "\n",
    "# Combine into one data frame\n",
    "df_keywords = pd.concat([df_post_keywords, df_thumbnail_keywords, df_article_keywords])\n",
    "\n",
    "# Discard posts with lower total reach to make charting easier\n",
    "df_keywords = df_keywords[df_keywords[\"Lifetime Post Total Reach\"] > 20000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='entity'></a>\n",
    "###  1.4 Create a Consolidated Entity DataFrame\n",
    "You'll create DataFrames for the entities of the article text, the thumbnail text, and the post text. Then you'll combine them into one DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Article Entity DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with only Article Keywords\n",
    "article_entities = [\"Text\", \"MaxLinkEntity\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "article_entities.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with keywords and metrics\n",
    "df_article_entities = df[article_entities]\n",
    "    \n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_article_entities[i] = pd.to_numeric(df_article_entities[i], errors='coerce')\n",
    "    \n",
    "# Drop NA Values in Keywords Column\n",
    "df_article_entities['MaxLinkEntity'] = df[\"MaxLinkEntity\"].replace(r'\\s+', np.nan, regex=True)\n",
    "df_article_entities.dropna(subset=['MaxLinkEntity'], inplace=True)\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_article_entities[\"Type\"] = \"Article\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thumbnail Entity DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with only Thumbnail Keywords\n",
    "thumbnail_entities = [\"Text\", \"MaxThumbnailEntity\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "thumbnail_entities.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with keywords and metrics\n",
    "df_thumbnail_entities = df[thumbnail_entities]\n",
    "\n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_thumbnail_entities[i] = pd.to_numeric(df_thumbnail_entities[i], errors='coerce')\n",
    "    \n",
    "# Drop NA Values in Keywords Column\n",
    "df_thumbnail_entities['MaxThumbnailEntity'] = df_thumbnail_entities['MaxThumbnailEntity'].replace(r'\\s+', np.nan, regex=True)\n",
    "df_thumbnail_entities.dropna(subset=['MaxThumbnailEntity'], inplace=True)\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_thumbnail_entities[\"Type\"] = \"Thumbnails\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Entity DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with only Thumbnail Keywords\n",
    "post_entities = [\"Text\", \"MaxTextEntity\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "post_entities.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with keywords and metrics\n",
    "df_post_entities = df[post_entities]\n",
    "\n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_post_entities[i] = pd.to_numeric(df_post_entities[i], errors='coerce')\n",
    "    \n",
    "# Drop NA Values in Keywords Column\n",
    "df_post_entities['MaxTextEntity'] = df_post_entities['MaxTextEntity'].replace(r'\\s+', np.nan, regex=True)\n",
    "df_post_entities.dropna(subset=['MaxTextEntity'], inplace=True)\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_post_entities[\"Type\"] = \"Posts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Post, Thumbnail, and Article DataFrames to Make One Entity DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make the column headers the same\n",
    "df_post_entities.rename(columns={\"MaxTextEntity\": \"Entities\"}, inplace=True)\n",
    "\n",
    "df_thumbnail_entities.rename(columns={\"MaxThumbnailEntity\":\"Entities\"}, inplace=True)\n",
    "\n",
    "df_article_entities.rename(columns={\"MaxLinkEntity\":\"Entities\"}, inplace=True)\n",
    "\n",
    "# Combine into one data frame\n",
    "df_entities = pd.concat([df_post_entities, df_thumbnail_entities, df_article_entities])\n",
    "\n",
    "df_entities[\"Entities\"] = df_entities[\"Entities\"].replace('', np.nan)\n",
    "df_entities.dropna(subset=[\"Entities\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='image_dataframe'></a>\n",
    "###  1.5 Create a Consolidated Image DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Metrics with Type Hierarchy, Class and Color to Make One Image DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visual_recognition:\n",
    "    # Create a list with only Visual Recognition columns\n",
    "    pic_keywords = ['Image Type', 'Image Subtype', 'Image Subtype2', 'Image Class', 'Image Color']\n",
    "\n",
    "    # Append DataFrame with these metrics\n",
    "    pic_keywords.extend(metrics)\n",
    "\n",
    "    # Create a new DataFrame with keywords and metrics\n",
    "    df_pic_keywords = df[pic_keywords]\n",
    "\n",
    "    # Make all metrics numeric\n",
    "    for i in metrics:\n",
    "        df_pic_keywords[i] = pd.to_numeric(df_pic_keywords[i], errors='coerce')\n",
    "\n",
    "    # Discard posts with lower total reach to make charting easier\n",
    "    df_pic_keywords = df_pic_keywords[df_pic_keywords[\"Lifetime Post Total Reach\"] > 15000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3\"></a> \n",
    "# Part III\n",
    "<a id='2setup'></a> \n",
    "## 1. Setup\n",
    "<a id='2setup2'></a>\n",
    "###  1.1 Assign Variables\n",
    "Assign new DataFrames to variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = df_entities\n",
    "tones = df_tones\n",
    "keywords = df_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2visual'></a>\n",
    "##  2. Visualize Data\n",
    "<a id='2visual1'></a> \n",
    "### 2.1 Run PixieDust Visualization Library with Display() API\n",
    "PixieDust lets you visualize your data in just a few clicks using the display() API. You can find more info at https://pixiedust.github.io/pixiedust/displayapi.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can use a pie chart to identify how lifetime engagement was broken up by sentiment. \n",
    "\n",
    "Click on the `Options` button to change the chart.  Here are some things to try:\n",
    "* Add *Type* to make the breakdown show *Post* or *Article*.\n",
    "* Show *Emotion* intead of *Sentiment* (or both).\n",
    "* Try a different metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "chartsize": "70",
      "charttype": "stacked",
      "clusterby": "Type",
      "handlerId": "pieChart",
      "keyFields": "Emotion",
      "legend": "true",
      "mpld3": "false",
      "orientation": "horizontal",
      "rendererId": "matplotlib",
      "rowCount": "100",
      "sortby": "Values DESC",
      "title": "Lifetime Engaged Users by Emotion",
      "valueFields": "Lifetime Engaged Users",
      "ylabel": "true"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(tones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at the same statistics as a bar chart.\n",
    "\n",
    "It is the same line of code. Use the `Edit Metadata` button to see how PixieDust knows to show us a bar chart. If you don't have a button use the menu and select `View > Cell Toolbar > Edit Metadata`.\n",
    "\n",
    "A bar chart is better at showing more information. We added `Cluster By: Type` so we already see numbers for posts and articles. Notice what the chart tells you. Most of our articles and posts are `positive`. But what sentiment really engages more users?  Click on `Options` and try this:\n",
    "\n",
    "* Change the aggregation to `AVG`.\n",
    "\n",
    "What sentiment leads to higher average engagement?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "chartsize": "70",
      "charttype": "stacked",
      "clusterby": "Type",
      "handlerId": "barChart",
      "keyFields": "Sentiment",
      "legend": "true",
      "orientation": "horizontal",
      "rendererId": "matplotlib",
      "rowCount": "100",
      "sortby": "Values DESC",
      "title": "Lifetime Engaged Users by Sentiment",
      "valueFields": "Lifetime Engaged Users"
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(tones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at the entities that were detected by Natural Language Understanding.\n",
    "\n",
    "The following bar chart shows the entities that were detected. This time we are stacking negative feedback and \"likes\" to get a picture of the kind of feedback the entities were getting. We chose a horizontal, stacked bar chart with descending values for a little variety.\n",
    "\n",
    "* Try a different renderer and see what you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "chartsize": "70",
      "charttype": "stacked",
      "handlerId": "barChart",
      "keyFields": "Entities",
      "orientation": "horizontal",
      "rendererId": "matplotlib",
      "rowCount": "100",
      "sortby": "Values DESC",
      "title": "Entities in Posts and Articles",
      "valueFields": "Lifetime Post Stories by action type - like,Lifetime Negative Feedback from Users"
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we look at the keywords detected by Natural Language Understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "chartsize": "85",
      "charttype": "stacked",
      "clusterby": "Type",
      "handlerId": "barChart",
      "keyFields": "Keywords",
      "legend": "true",
      "mpld3": "false",
      "orientation": "horizontal",
      "rendererId": "matplotlib",
      "rowCount": "100",
      "sortby": "Values DESC",
      "timeseries": "false",
      "title": "Keyword Total Reach",
      "valueFields": "Lifetime Post Total Reach"
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's take a look at what Visual Recognition can show us.\n",
    "\n",
    "See how the images influenced the metrics. We've used visual recognition to identify a class and a type hierarchy for each image. We've also captured the top recognized color for each image. Our sample data doesn't have a significant number of data points, but these three charts demonstrate how you could:\n",
    "\n",
    "1. Recognize image classes that correlate to higher total reach.\n",
    "1. Add a type hierarchy for a higher level abstraction or to add grouping/stacking to the class data.\n",
    "1. Determine if image color correlates to total reach.\n",
    "\n",
    "Visual recognition makes it surprisingly easy to do all of the above. Of course, you can easily try different metrics as you experiment. If you are not convinced that you should add ultramarine laser pictures to all of your articles, then you might want to do some research with a better data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "chartsize": "85",
      "charttype": "stacked",
      "handlerId": "barChart",
      "keyFields": "Image Class",
      "legend": "true",
      "orientation": "horizontal",
      "rendererId": "matplotlib",
      "rowCount": "100",
      "sortby": "Values ASC",
      "title": "Image Classes",
      "valueFields": "Lifetime Post Total Reach"
     }
    }
   },
   "outputs": [],
   "source": [
    "if visual_recognition:\n",
    "    display(df_pic_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "chartsize": "85",
      "charttype": "stacked",
      "handlerId": "barChart",
      "keyFields": "Image Subtype",
      "legend": "true",
      "mpld3": "false",
      "orientation": "horizontal",
      "rendererId": "matplotlib",
      "rowCount": "100",
      "sortby": "Values ASC",
      "stretch": "false",
      "title": "Image Type Hierarchy",
      "valueFields": "Lifetime Post Total Reach"
     }
    }
   },
   "outputs": [],
   "source": [
    "if visual_recognition:\n",
    "    display(df_pic_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "chartsize": "85",
      "charttype": "stacked",
      "handlerId": "barChart",
      "keyFields": "Image Color",
      "legend": "true",
      "orientation": "horizontal",
      "rendererId": "matplotlib",
      "rowCount": "100",
      "sortby": "Values ASC",
      "title": "Image Color",
      "valueFields": "Lifetime Post Total Reach"
     }
    }
   },
   "outputs": [],
   "source": [
    "if visual_recognition:\n",
    "    display(df_pic_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Copyright  IBM Corp. 2017, 2018. This notebook and its source code are released under the terms of the Apache 2.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n",
    "not use this file except in compliance with the License. You may obtain\n",
    "a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
    "WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
    "License for the specific language governing permissions and limitations\n",
    "under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
