{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n",
    "# not use this file except in compliance with the License. You may obtain\n",
    "# a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
    "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
    "# License for the specific language governing permissions and limitations\n",
    "# under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analyze Facebook Data Using IBM Watson and IBM Data Platform\n",
    "\n",
    "This is a three-part notebook written in `Python_3.5` meant to show how anyone can enrich and analyze a combined dataset of unstructured and structured information with IBM Watson and IBM Data Platform. For this example we are using a standard Facebook Analytics export which features texts from posts, articles and thumbnails, along with standard performance metrics such as likes, shares, and impressions. \n",
    "\n",
    "**Part I** will use the Natual Language Understanding, Visual Recognition and Tone Analyzer Services from IBM Watson to enrich the Facebook Posts, Thumbnails, and Articles by pulling out `Emotion Tones`, `Social Tones`, `Language Tones`, `Entities`, `Keywords`, and `Document Sentiment`. The end result of Part I will be additional features and metrics we can test, analyze, and visualize in Part III. \n",
    "\n",
    "**Part II** will be used to set up the visualizations and tests we will run in Part III. The end result of Part II will be multiple pandas DataFrames that will contain the values, and metrics needed to find insights from the Part III tests and experiments.\n",
    "\n",
    "**Part III** will include services from IBM's Data Platform, including IBM's own data visualization library PixieDust. In Part III we will run analysis on the data from the Facebook Analytics export, such as the number of likes, comments, shares, to the overall reach for each post, and will compare it to the enriched data we pulled in Part I.\n",
    "\n",
    "\n",
    "#### You should only need to change data in the Setup portion of this notebook. All places where you see  <span style=\"color: red\"> User Input </span> is where you should be adding inputs. \n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "### [**Part I - Enrich**](#part1)<br>\n",
    "1. [Setup](#setup)<br>\n",
    "   1.1 [Install Watson Developer Cloud and BeautifulSoup Packages](#setup1)<br>\n",
    "   1.2 [Install PixieDust](#pixie)<br> \n",
    "   1.3 [Restart Kernel](#restart)<br> \n",
    "   1.4 [Import Packages and Libraries](#setup2)<br>\n",
    "   1.5 [Add Service Credentials From Bluemix for Watson Services](#setup3)<br>\n",
    "2. [Load Data](#load)<br>\n",
    "   2.1 [Load Data From SoftLayer's Object Storage as a Pandas DataFrame](#load1)<br>\n",
    "3. [Prepare Data](#prepare)<br>\n",
    "   3.1 [Data Cleansing with Python](#prepare1)<br>\n",
    "   3.2 [Beautiful Soup to Extract Thumbnails and Extented Links](#prepare2)<br>\n",
    "4. [Enrich Data](#enrich)<br>\n",
    "   4.1 [NLU for Post Text](#nlupost)<br>\n",
    "   4.2 [NLU for Thumbnail Text](#nlutn)<br>\n",
    "   4.3 [NLU for Article Text](#nlulink)<br>\n",
    "   4.4 [Tone Analyzer for Post Text](#tonepost)<br>\n",
    "   4.5 [Tone Analyzer for Article Text](#tonearticle)<br>\n",
    "   4.6 [Visual Recognition](#visual)<br>\n",
    "5. [Write Data](#write)<br>\n",
    "   5.1 [Convert DataFrame to new CSV](#write1)<br>\n",
    "   5.2 [Write Data to SoftLayer's Object Storage](#write2)<br>\n",
    "    \n",
    "### [**Part II - Data Preparation**](#part2)<br>\n",
    "1. [Prepare Data](#prepare)<br>\n",
    "   1.1 [Create Multiple DataFrames for Visualizations](#visualizations)<br>\n",
    "   1.2 [Create a Consolidated Tone DataFrame](#tone)<br>\n",
    "   1.3 [Create a Consolidated Keyword DataFrame](#keyword)<br>\n",
    "   1.4 [Create a Consolidated Entity DataFrame](#entity)<br>\n",
    "  \n",
    "### [**Part III - Analyze**](#part3)<br>\n",
    "\n",
    "1. [Setup](#2setup)<br> \n",
    "    1.1 [Assign Variables](#2setup2)<br>\n",
    "2. [Visualize Data](#2visual)<br>\n",
    "    2.1 [Run PixieDust Visualization Library with Display() API](#2visual1)\n",
    "   \n",
    "### Learn more about the technology used:\n",
    "\n",
    "* [Natual Language Understanding](https://www.ibm.com/watson/developercloud/natural-language-understanding.html)\n",
    "* [Tone Analyzer](https://www.ibm.com/watson/developercloud/tone-analyzer.html)\n",
    "* [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [PixieDust](https://github.com/ibm-cds-labs/pixiedust) (Part III)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Documents\n",
    "[Sample Facebook Posts](https://github.com/IBM/pixiedust-facebook-analysis/blob/master/data/example_input/example_facebook_data.csv) - This is a sample export of IBM Watson's Facebook Page. Engagement metrics such as clicks, impressions, etc. are all changed and do not reflect any actual post performance data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "#  Part I - Enrich\n",
    "\n",
    "<a id=\"setup\"></a>\n",
    "## 1. Setup\n",
    "<a id=\"setup1\"></a>\n",
    "### 1.1 Install Latest Watson Developer Cloud and Beautiful Soup Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade watson-developer-cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pixie\"></a>\n",
    "### 1.2 Install PixieDust Library\n",
    "This notebook provides an overview of how to use the PixieDust library to analyze and visualize various data sets. If you are new to PixieDust or would like to learn more about the library, please go to this [Introductory Notebook](https://apsportal.ibm.com/exchange/public/entry/view/5b000ed5abda694232eb5be84c3dd7c1) or visit the [PixieDust Github](https://ibm-cds-labs.github.io/pixiedust/). The `Setup` section for this notebook uses instructions from the [Intro To PixieDust](https://github.com/ibm-cds-labs/pixiedust/blob/master/notebook/Intro%20to%20PixieDust.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you are running the latest version of PixieDust by running the following cell. Do not run this cell if you installed PixieDust locally from source and want to continue to run PixieDust from source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user --upgrade pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"restart\"></a>\n",
    "### 1.3 Restart Kernel\n",
    "> Required after installs/upgrades only.\n",
    "\n",
    "If any libraries were just installed or upgraded, <span style=\"color: red\">restart the kernel</span> before continuing. After this has been done once, you might want to comment out the `!pip install` lines above for cleaner output and a faster \"Run All\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup2\"></a>\n",
    "### 1.4 Import Packages and Libraries\n",
    "Tip: To check if you have a package installed, open a new cell and write: `help(<package-name>)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "import watson_developer_cloud\n",
    "from watson_developer_cloud import ToneAnalyzerV3, VisualRecognitionV3\n",
    "import watson_developer_cloud.natural_language_understanding.features.v1 as features\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from operator import itemgetter\n",
    "from os.path import join, dirname\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pixiedust\n",
    "\n",
    "# Suppress some pandas warnings\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup3'></a>\n",
    "### 1.5 Add Service Credentials From Bluemix for Watson Services\n",
    "Edit the following cell to provide your credentials for Watson Visual Recognition, Natural Language Understanding and Tone Analyzer.\n",
    "\n",
    "###  <span style=\"color: red\"> _User Input_</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "\n",
    "# Watson Visual Recognition\n",
    "VISUAL_RECOGNITION_API_KEY = '<add_vr_api_key>'\n",
    "\n",
    "# Watson Natural Launguage Understanding (NLU)\n",
    "NATURAL_LANGUAGE_UNDERSTANDING_USERNAME = '<add_nlu_username>'\n",
    "NATURAL_LANGUAGE_UNDERSTANDING_PASSWORD = '<add_nlu_password>'\n",
    "\n",
    "# Watson Tone Analyzer\n",
    "TONE_ANALYZER_USERNAME = '<add_tone_analyzer_username>'\n",
    "TONE_ANALYZER_PASSWORD = '<add_tone_analyzer_password>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Watson clients\n",
    "\n",
    "nlu = watson_developer_cloud.NaturalLanguageUnderstandingV1(version='2017-02-27',\n",
    "                                                            username=NATURAL_LANGUAGE_UNDERSTANDING_USERNAME,\n",
    "                                                            password=NATURAL_LANGUAGE_UNDERSTANDING_PASSWORD)\n",
    "tone_analyzer = ToneAnalyzerV3(version='2016-05-19',\n",
    "                               username=TONE_ANALYZER_USERNAME,\n",
    "                               password=TONE_ANALYZER_PASSWORD)\n",
    "\n",
    "visual_recognition = VisualRecognitionV3('2016-05-20', api_key=VISUAL_RECOGNITION_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load'></a> \n",
    "## 2. Load Data\n",
    "\n",
    "### 2.1 Load data from Object Storage\n",
    "\n",
    "**Select the cell below and place your cursor on an empty line below the comment.** Load the CSV file you want to enrich by clicking on the 10/01 icon (upper right), then click `Insert to code` under the file you want to enrich, and choose `Insert Pandas DataFrame`.\n",
    "\n",
    "###  <span style=\"color: red\"> _User Input_</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure this uses the variable above. The number will vary in the inserted code.\n",
    "try:\n",
    "    df = df_data_1\n",
    "except NameError as e:\n",
    "    print('Error: Setup is incorrect or incomplete.\\n')\n",
    "    print('Follow the instructions to insert the pandas DataFrame above, and edit to')\n",
    "    print('make the generated df_data_# variable match the variable used here.')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the cell below and place your cursor on an empty line below the comment.** \n",
    "Put in the credentials for the file you want to enrich by clicking on the 10/01 (upper right), then click `Insert to code` under the file you want to enrich, and choose `Insert Credentials`.\n",
    "\n",
    "**Change the inserted variable name to `credentials_1`**\n",
    "\n",
    "###  <span style=\"color: red\"> _User Input_</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#insert credentials for file - Change to credentials_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure this uses the variable above. The number will vary in the inserted code.\n",
    "try:\n",
    "    credentials = credentials_1\n",
    "except NameError as e:\n",
    "    print('Error: Setup is incorrect or incomplete.\\n')\n",
    "    print('Follow the instructions to insert the file credentials above, and edit to')\n",
    "    print('make the generated credentials_# variable match the variable used here.')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prepare'></a>\n",
    "## 3. Prepare Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prepare1'></a>\n",
    "###  3.1 Data Cleansing with Python\n",
    "Renaming columns, removing noticeable noise in the data, pulling out URLs and appending to a new column to run through NLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Post Message': 'Text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows that have NaN for the text.\n",
    "df.dropna(subset=['Text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_http = df[\"Text\"].str.partition(\"http\")\n",
    "df_www = df[\"Text\"].str.partition(\"www\")\n",
    "\n",
    "#combine delimiters with actual links\n",
    "df_http[\"Link\"] = df_http[1].map(str) + df_http[2]\n",
    "df_www[\"Link1\"] = df_www[1].map(str) + df_www[2]\n",
    "\n",
    "#include only Link columns \n",
    "df_http.drop(df_http.columns[0:3], axis=1, inplace = True)\n",
    "df_www.drop(df_www.columns[0:3], axis=1, inplace = True)\n",
    "\n",
    "#merge http and www DataFrames\n",
    "dfmerge = pd.concat([df_http, df_www], axis=1)\n",
    "\n",
    "#the following steps will allow you to merge data columns from the left to the right\n",
    "dfmerge = dfmerge.apply(lambda x: x.str.strip()).replace('', np.nan)\n",
    "\n",
    "#use fillna to fill any blanks with the Link1 column\n",
    "dfmerge[\"Link\"].fillna(dfmerge[\"Link1\"], inplace = True)\n",
    "\n",
    "#delete Link1 (www column)\n",
    "dfmerge.drop(\"Link1\", axis=1, inplace = True)\n",
    "\n",
    "#combine Link data frame \n",
    "df = pd.concat([dfmerge,df], axis = 1)\n",
    "\n",
    "# # make sure text column is a string\n",
    "df[\"Text\"] = df[\"Text\"].astype(\"str\")\n",
    "\n",
    "# #strip links from Text column\n",
    "df['Text'] = df['Text'].apply(lambda x: x.split('http')[0])\n",
    "df['Text'] = df['Text'].apply(lambda x: x.split('www')[0])\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull thumbnail descriptions and image urls using beautiful soup\n",
    "\n",
    "# Change links from objects to strings\n",
    "for link in df.Link:\n",
    "    df.Link.to_string()\n",
    "\n",
    "piclinks = []\n",
    "description = []\n",
    "for url in df[\"Link\"]:\n",
    "    if pd.isnull(url):\n",
    "        piclinks.append(\"\")\n",
    "        description.append(\"\")\n",
    "        continue    \n",
    "        \n",
    "    page3 = requests.get(url)\n",
    "    if page3.status_code != requests.codes.ok:\n",
    "        # print(page3.status_code)\n",
    "        piclinks.append(\"\")\n",
    "        description.append(\"\")\n",
    "        continue   \n",
    "        \n",
    "    soup3 = bs(page3.text,\"lxml\")\n",
    "    \n",
    "    pic = soup3.find('meta', property =\"og:image\")\n",
    "    if pic:\n",
    "        piclinks.append(pic[\"content\"])\n",
    "    else: \n",
    "        piclinks.append(\"\")\n",
    "    \n",
    "    content = None\n",
    "    desc = soup3.find(attrs={'name':'Description'})\n",
    "    if desc:\n",
    "        content = desc['content']\n",
    "    if not content or content == 'null':\n",
    "        # Try again with lowercase description\n",
    "        desc = soup3.find(attrs={'name':'description'})\n",
    "        if desc:\n",
    "            content = desc['content']\n",
    "    if not content or content == 'null':\n",
    "        description.append(\"\")\n",
    "    else:\n",
    "        description.append(content)\n",
    "            \n",
    "# Save thumbnail descriptions to df in a column titled 'Thumbnails'\n",
    "df[\"Thumbnails\"] = description\n",
    "# Save image links to df in a column titled 'Image'\n",
    "df[\"Image\"] = piclinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert shortened links to full links.\n",
    "Use requests module to pull extended lists. This is only necessary if the Facebook page uses different links than the articles themselves. For this example we are using IBM Watson's Facebook export which uses an IBM link. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts shortened links to their original form\n",
    "\n",
    "shortlink = df[\"Link\"]\n",
    "extendedlink = []\n",
    "\n",
    "for link in shortlink:\n",
    "    if isinstance(link, float):  # Float is not a URL, probably NaN.\n",
    "        extendedlink.append('')\n",
    "    else:\n",
    "        extended_link = requests.Session().head(link, allow_redirects=True).url\n",
    "        extendedlink.append(extended_link)\n",
    "\n",
    "df[\"Extended Links\"] = extendedlink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='enrich'></a> \n",
    "## 4. Enrichment Time!\n",
    "<a id='nlupost'></a>\n",
    "###  4.1 NLU for the Post Text\n",
    "Below uses Natural Language Understanding to iterate through each post and extract the enrichment features we want to use in our future analysis.\n",
    "\n",
    "Each feature we extract will be appended to the `.csv` in a new column we determine at the end of this script. If you want to run this same script for the other columns, define `free_form_responses` to the column name, if you are using URLs, change `text=response` parameter to `url=response`, and update the new column names as you see fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the free form text response from the data frame\n",
    "# If you are using this script for a diff CSV, you will have to change this column name\n",
    "free_form_responses = df['Text']\n",
    "# define the list of enrichments to apply\n",
    "# if you are modifying this script add or remove the enrichments as needed\n",
    "f = [features.Entities(), features.Keywords(),features.Emotion(),features.Sentiment()]\n",
    "\n",
    "# Create a list to store the enriched data\n",
    "overallSentimentScore = []\n",
    "overallSentimentType = []\n",
    "highestEmotion = []\n",
    "highestEmotionScore = []\n",
    "kywords = []\n",
    "entities = []\n",
    "\n",
    "# Go through every response and enrich the text using NLU.\n",
    "for response in free_form_responses:\n",
    "    # We are assuming English to avoid errors when the language cannot be detected.\n",
    "    enriched_json = nlu.analyze(text=response, features=f, language='en')\n",
    "    #print(enriched_json)\n",
    "\n",
    "    # get the SENTIMENT score and type\n",
    "    if 'sentiment' in enriched_json:\n",
    "        if('score' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentScore.append(enriched_json[\"sentiment\"][\"document\"][\"score\"])\n",
    "        else:\n",
    "            overallSentimentScore.append('0')\n",
    "\n",
    "        if('label' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentType.append(enriched_json[\"sentiment\"][\"document\"][\"label\"])\n",
    "        else:\n",
    "            overallSentimentType.append('0')\n",
    "    else:\n",
    "        overallSentimentScore.append('0')\n",
    "        overallSentimentType.append('0')\n",
    "\n",
    "    # read the EMOTIONS into a dict and get the key (emotion) with maximum value\n",
    "    if 'emotion' in enriched_json:\n",
    "        me = max(enriched_json[\"emotion\"][\"document\"][\"emotion\"].items(), key=operator.itemgetter(1))[0]\n",
    "        highestEmotion.append(me)\n",
    "        highestEmotionScore.append(enriched_json[\"emotion\"][\"document\"][\"emotion\"][me])\n",
    "\n",
    "    else:\n",
    "        highestEmotion.append(\"\")\n",
    "        highestEmotionScore.append(\"\")\n",
    "\n",
    "    #iterate and get KEYWORDS with a confidence of over 50%\n",
    "    if 'keywords' in enriched_json:\n",
    "        #print((enriched_json['keywords']))\n",
    "        tmpkw = []\n",
    "        for kw in enriched_json['keywords']:\n",
    "            if(float(kw[\"relevance\"]) >= 0.5):\n",
    "                #print(\"kw is: \", kw, \"and val is \", kw[\"text\"])\n",
    "                tmpkw.append(kw[\"text\"])#str(kw[\"text\"]).strip('[]')\n",
    "        #convert multiple keywords in a list to a string\n",
    "        if(len(tmpkw) > 1):\n",
    "            tmpkw = \"\".join(reduce(lambda a, b: a + ', ' + b, tmpkw))\n",
    "        elif(len(tmpkw) == 0):\n",
    "            tmpkw = \"\"\n",
    "        else:\n",
    "            tmpkw = \"\".join(reduce(lambda a, b='': a + b , tmpkw))\n",
    "        kywords.append(tmpkw)\n",
    "    else:\n",
    "        kywords.append(\"\")\n",
    "            \n",
    "    #iterate and get Entities with a confidence of over 30%\n",
    "    if 'entities' in enriched_json:\n",
    "        #print((enriched_json['entities']))\n",
    "        tmpent = []\n",
    "        for ent in enriched_json['entities']:\n",
    "                \n",
    "            if(float(ent[\"relevance\"]) >= 0.3):\n",
    "                tmpent.append(ent[\"type\"])\n",
    "        #convert multiple concepts in a list to a string\n",
    "        if(len(tmpent) > 1):\n",
    "            tmpent = \"\".join(reduce(lambda a, b: a + ', ' + b, tmpent))\n",
    "        elif(len(tmpent) == 0):\n",
    "            tmpent = \"\"\n",
    "        else:\n",
    "            tmpent = \"\".join(reduce(lambda a, b='': a + b , tmpent))\n",
    "        entities.append(tmpent)\n",
    "    else:\n",
    "        entities.append(\"\")    \n",
    "    \n",
    "# Create columns from the list and append to the DataFrame\n",
    "if highestEmotion:\n",
    "    df['TextHighestEmotion'] = highestEmotion\n",
    "if highestEmotionScore:\n",
    "    df['TextHighestEmotionScore'] = highestEmotionScore\n",
    "\n",
    "if overallSentimentType:\n",
    "    df['TextOverallSentimentType'] = overallSentimentType\n",
    "if overallSentimentScore:\n",
    "    df['TextOverallSentimentScore'] = overallSentimentScore\n",
    "\n",
    "df['TextKeywords'] = kywords\n",
    "df['TextEntities'] = entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we extract all of the Keywords and Entities from each Post, we have a column with multiple Keywords, and Entities separated by commas. For our Analysis in Part II we wanted also wanted the top Keyword and Entity for each Post. Because of this, we added two new columns to capture the `MaxTextKeyword` and `MaxTextEntity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose first of Keywords,Concepts, Entities\n",
    "df[\"MaxTextKeywords\"] = df[\"TextKeywords\"].apply(lambda x: x.split(',')[0])\n",
    "df[\"MaxTextEntity\"] = df[\"TextEntities\"].apply(lambda x: x.split(',')[0])\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nlutn'></a>\n",
    "###  4.2 NLU for Thumbnail Text\n",
    "\n",
    "We will repeat the same process for Thumbnails and Article Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the thumbnail text from the data frame\n",
    "# If you are using this script for a diff CSV, you will have to change this column name\n",
    "free_form_responses= df['Thumbnails']\n",
    "# define the list of enrichments to apply\n",
    "# if you are modifying this script add or remove the enrichments as needed\n",
    "f = [features.Entities(), features.Keywords(),features.Emotion(),features.Sentiment()]#\n",
    "\n",
    "# Create a list to store the enriched data\n",
    "overallSentimentScore = []\n",
    "overallSentimentType = []\n",
    "highestEmotion = []\n",
    "highestEmotionScore = []\n",
    "kywords = []\n",
    "entities = []\n",
    "\n",
    "\n",
    "# Go thru every reponse and enrich the text using NLU\n",
    "for response in free_form_responses:\n",
    "    if not response:\n",
    "        overallSentimentScore.append(' ')\n",
    "        overallSentimentType.append(' ')\n",
    "        highestEmotion.append(' ')\n",
    "        highestEmotionScore.append(' ')\n",
    "        kywords.append(' ')\n",
    "        entities.append(' ')\n",
    "        continue\n",
    "\n",
    "    enriched_json = nlu.analyze(text=response, features=f)\n",
    "    #print(enriched_json)\n",
    "\n",
    "    # get the SENTIMENT score and type\n",
    "    if 'sentiment' in enriched_json:\n",
    "        if('score' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentScore.append(enriched_json[\"sentiment\"][\"document\"][\"score\"])\n",
    "        else:\n",
    "            overallSentimentScore.append(\"\")\n",
    "\n",
    "        if('label' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentType.append(enriched_json[\"sentiment\"][\"document\"][\"label\"])\n",
    "        else:\n",
    "            overallSentimentType.append(\"\")\n",
    "\n",
    "    # read the EMOTIONS into a dict and get the key (emotion) with maximum value\n",
    "    if 'emotion' in enriched_json:\n",
    "        me = max(enriched_json[\"emotion\"][\"document\"][\"emotion\"].items(), key=operator.itemgetter(1))[0]\n",
    "        highestEmotion.append(me)\n",
    "        highestEmotionScore.append(enriched_json[\"emotion\"][\"document\"][\"emotion\"][me])\n",
    "\n",
    "    else:\n",
    "        highestEmotion.append(\"\")\n",
    "        highestEmotionScore.append(\"\")\n",
    "\n",
    "    #iterate and get KEYWORDS with a confidence of over 50%\n",
    "    if 'keywords' in enriched_json:\n",
    "        #print((enriched_json['keywords']))\n",
    "        tmpkw = []\n",
    "        for kw in enriched_json['keywords']:\n",
    "            if(float(kw[\"relevance\"]) >= 0.5):\n",
    "                #print(\"kw is: \", kw, \"and val is \", kw[\"text\"])\n",
    "                tmpkw.append(kw[\"text\"])#str(kw[\"text\"]).strip('[]')\n",
    "        #convert multiple keywords in a list to a string\n",
    "        if(len(tmpkw) > 1):\n",
    "            tmpkw = \"\".join(reduce(lambda a, b: a + ', ' + b, tmpkw))\n",
    "        elif(len(tmpkw) == 0):\n",
    "            tmpkw = \"\"\n",
    "        else:\n",
    "            tmpkw = \"\".join(reduce(lambda a, b='': a + b , tmpkw))\n",
    "        kywords.append(tmpkw)\n",
    "     \n",
    "    #iterate and get Entities with a confidence of over 30%\n",
    "    if 'entities' in enriched_json:\n",
    "        #print((enriched_json['entities']))\n",
    "        tmpent = []\n",
    "        for ent in enriched_json['entities']:\n",
    "                \n",
    "            if(float(ent[\"relevance\"]) >= 0.3):\n",
    "                tmpent.append(ent[\"type\"])\n",
    "        #convert multiple concepts in a list to a string\n",
    "        if(len(tmpent) > 1):\n",
    "            tmpent = \"\".join(reduce(lambda a, b: a + ', ' + b, tmpent))\n",
    "        elif(len(tmpent) == 0):\n",
    "            tmpent = \"\"\n",
    "        else:\n",
    "            tmpent = \"\".join(reduce(lambda a, b='': a + b , tmpent))\n",
    "        entities.append(tmpent)\n",
    "    else:\n",
    "        entities.append(\"\")     \n",
    "\n",
    "# print(len(highestEmotion))\n",
    "# print(len(highestEmotionScore))\n",
    "# print(len(overallSentimentType))\n",
    "# print(len(overallSentimentScore))\n",
    "# print(len(kywords))\n",
    "# print(len(entities))\n",
    "    \n",
    "# Create columns from the list and append to the DataFrame\n",
    "if highestEmotion:\n",
    "    df['ThumbnailHighestEmotion'] = highestEmotion\n",
    "if highestEmotionScore:\n",
    "    df['ThumbnailHighestEmotionScore'] = highestEmotionScore\n",
    "\n",
    "if overallSentimentType:\n",
    "    df['ThumbnailOverallSentimentType'] = overallSentimentType\n",
    "if overallSentimentScore:\n",
    "    df['ThumbnailOverallSentimentScore'] = overallSentimentScore\n",
    "\n",
    "df['ThumbnailKeywords'] = kywords\n",
    "df['ThumbnailEntities'] = entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose first of Keywords,Concepts,Entities\n",
    "df[\"MaxThumbnailKeywords\"] = df[\"ThumbnailKeywords\"].apply(lambda x: x.split(',')[0])\n",
    "df[\"MaxThumbnailEntity\"] = df[\"ThumbnailEntities\"].apply(lambda x: x.split(',')[0])\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nlulink'></a> \n",
    "### 4.3 NLU for Article Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_form_responses = df['Extended Links']\n",
    "f = [features.Entities(), features.Keywords(),features.Emotion(),features.Sentiment()]\n",
    "\n",
    "# Create a list to store the enriched data\n",
    "overallSentimentScore = []\n",
    "overallSentimentType = []\n",
    "highestEmotion = []\n",
    "highestEmotionScore = []\n",
    "kywords = []\n",
    "entities = []\n",
    "article_text = []\n",
    "        \n",
    "# Go through every response and enrich the text using NLU\n",
    "for response in free_form_responses:\n",
    "    if not response:\n",
    "        overallSentimentScore.append(' ')\n",
    "        overallSentimentType.append(' ')\n",
    "        highestEmotion.append(' ')\n",
    "        highestEmotionScore.append(' ')\n",
    "        kywords.append(' ')\n",
    "        entities.append(' ')\n",
    "        article_text.append(' ')\n",
    "        continue\n",
    "\n",
    "    # Run links through NLU to get entities, keywords, emotion and sentiment.\n",
    "    # Use return_analyzed_text to extract text for Tone Analyzer to use.\n",
    "    enriched_json = nlu.analyze(url=response, features=f, return_analyzed_text=True)\n",
    "    #print(enriched_json)\n",
    "    \n",
    "    article_text.append(enriched_json[\"analyzed_text\"])\n",
    "\n",
    "    # get the SENTIMENT score and type\n",
    "    if 'sentiment' in enriched_json:\n",
    "        if('score' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentScore.append(enriched_json[\"sentiment\"][\"document\"][\"score\"])\n",
    "        else:\n",
    "            overallSentimentScore.append('None')\n",
    "\n",
    "        if('label' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentType.append(enriched_json[\"sentiment\"][\"document\"][\"label\"])\n",
    "        else:\n",
    "            overallSentimentType.append('')\n",
    "\n",
    "    # read the EMOTIONS into a dict and get the key (emotion) with maximum value\n",
    "    if 'emotion' in enriched_json:\n",
    "        me = max(enriched_json[\"emotion\"][\"document\"][\"emotion\"].items(), key=operator.itemgetter(1))[0]\n",
    "        highestEmotion.append(me)\n",
    "        highestEmotionScore.append(enriched_json[\"emotion\"][\"document\"][\"emotion\"][me])\n",
    "\n",
    "    else:\n",
    "        highestEmotion.append('')\n",
    "        highestEmotionScore.append('')\n",
    "\n",
    "    #iterate and get KEYWORDS with a confidence of over 50%\n",
    "    if 'keywords' in enriched_json:\n",
    "        #print((enriched_json['keywords']))\n",
    "        tmpkw = []\n",
    "        for kw in enriched_json['keywords']:\n",
    "            if(float(kw[\"relevance\"]) >= 0.5):\n",
    "                #print(\"kw is: \", kw, \"and val is \", kw[\"text\"])\n",
    "                tmpkw.append(kw[\"text\"])#str(kw[\"text\"]).strip('[]')\n",
    "        #convert multiple keywords in a list to a string\n",
    "        if(len(tmpkw) > 1):\n",
    "            tmpkw = \"\".join(reduce(lambda a, b: a + ', ' + b, tmpkw))\n",
    "        elif(len(tmpkw) == 0):\n",
    "            tmpkw = \"\"\n",
    "        else:\n",
    "            tmpkw = \"\".join(reduce(lambda a, b='': a + b , tmpkw))\n",
    "        kywords.append(tmpkw)\n",
    "    else: \n",
    "        kywords.append(\"\")\n",
    "            \n",
    "    #iterate and get Entities with a confidence of over 30%\n",
    "    if 'entities' in enriched_json:\n",
    "        #print((enriched_json['entities']))\n",
    "        tmpent = []\n",
    "        for ent in enriched_json['entities']:\n",
    "                \n",
    "            if(float(ent[\"relevance\"]) >= 0.3):\n",
    "                tmpent.append(ent[\"type\"])\n",
    "        #convert multiple concepts in a list to a string\n",
    "        if(len(tmpent) > 1):\n",
    "            tmpent = \"\".join(reduce(lambda a, b: a + ', ' + b, tmpent))\n",
    "        elif(len(tmpent) == 0):\n",
    "            tmpent = \"\"\n",
    "        else:\n",
    "            tmpent = \"\".join(reduce(lambda a, b='': a + b , tmpent))\n",
    "        entities.append(tmpent)\n",
    "    else:\n",
    "        entities.append(\"\")\n",
    "\n",
    "# print(len(highestEmotion))\n",
    "# print(len(highestEmotionScore))\n",
    "# print(len(overallSentimentType))\n",
    "# print(len(overallSentimentScore))\n",
    "# print(len(kywords))\n",
    "\n",
    "# print(len(entities))\n",
    "    \n",
    "# Create columns from the list and append to the DataFrame\n",
    "if highestEmotion:\n",
    "    df['LinkHighestEmotion'] = highestEmotion\n",
    "if highestEmotionScore:\n",
    "    df['LinkHighestEmotionScore'] = highestEmotionScore\n",
    "\n",
    "if overallSentimentType:\n",
    "    df['LinkOverallSentimentType'] = overallSentimentType\n",
    "if overallSentimentScore:\n",
    "    df['LinkOverallSentimentScore'] = overallSentimentScore\n",
    "\n",
    "df['LinkKeywords'] = kywords\n",
    "# df['TextConcepts'] = concepts\n",
    "df['LinkEntities'] = entities\n",
    "df['Article Text'] = article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"MaxLinkKeywords\"] = df[\"LinkKeywords\"].apply(lambda x: x.split(',')[0])\n",
    "df[\"MaxLinkEntity\"] = df[\"LinkEntities\"].apply(lambda x: x.split(',')[0])\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='tonepost'></a> \n",
    "### 4.4 Tone Analyzer for Post Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the free form text response from the data frame\n",
    "# If you are using this script for a diff CSV, you will have to change this column name\n",
    "free_form_responses = df['Text']\n",
    "\n",
    "#Create a list to store the enriched data\n",
    "\n",
    "highestEmotionTone = []\n",
    "emotionToneScore = []\n",
    "\n",
    "languageToneScore = []\n",
    "highestLanguageTone = []\n",
    "\n",
    "socialToneScore = []\n",
    "highestSocialTone = []\n",
    "\n",
    "\n",
    "for response in free_form_responses:\n",
    "    enriched_json = tone_analyzer.tone(text=response)\n",
    "    #print(enriched_json)\n",
    "        \n",
    "    if 'tone_categories' in enriched_json['document_tone']:\n",
    "        me = max(enriched_json[\"document_tone\"][\"tone_categories\"][0][\"tones\"], key = itemgetter('score'))['tone_name']      \n",
    "        highestEmotionTone.append(me)\n",
    "        you = max(enriched_json[\"document_tone\"][\"tone_categories\"][0][\"tones\"], key = itemgetter('score'))['score']\n",
    "        emotionToneScore.append(you)\n",
    "            \n",
    "        me1 = max(enriched_json[\"document_tone\"][\"tone_categories\"][1][\"tones\"], key = itemgetter('score'))['tone_name']      \n",
    "        highestLanguageTone.append(me1)\n",
    "        you1 = max(enriched_json[\"document_tone\"][\"tone_categories\"][1][\"tones\"], key = itemgetter('score'))['score']\n",
    "        languageToneScore.append(you1)\n",
    "            \n",
    "        me2 = max(enriched_json[\"document_tone\"][\"tone_categories\"][2][\"tones\"], key = itemgetter('score'))['tone_name']      \n",
    "        highestSocialTone.append(me2)\n",
    "        you2 = max(enriched_json[\"document_tone\"][\"tone_categories\"][2][\"tones\"], key = itemgetter('score'))['score']\n",
    "        socialToneScore.append(you2)\n",
    "    \n",
    "if highestEmotionTone:\n",
    "    df['highestEmotionTone'] = highestEmotionTone    \n",
    "if emotionToneScore:\n",
    "    df['emotionToneScore'] = emotionToneScore\n",
    "    \n",
    "if languageToneScore:\n",
    "    df['languageToneScore'] = languageToneScore\n",
    "if highestLanguageTone:\n",
    "    df['highestLanguageTone'] = highestLanguageTone\n",
    "    \n",
    "if highestSocialTone:\n",
    "    df['highestSocialTone'] = highestSocialTone    \n",
    "if socialToneScore:\n",
    "    df['socialToneScore'] = socialToneScore \n",
    "    \n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='enrich2'></a> \n",
    "### 4.5 Tone Analyzer for Article Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike NLU, Tone Analyzer cannot iterate through a URL so here we use NLU to pull the Article Text from the URL and append it to the original DataFrame. \n",
    "\n",
    "To do this, we pull out the `MetaData` feature, and make sure the `return_analyzed_text` parameter is set to `True`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the script for NLU, we are now using Tone Analyzer to iterate through the newly created and appended `Article Text` column which contains all of the free form text from the articles contained in the Facebook posts. \n",
    "\n",
    "We are using Tone Analyzer to gather the top Social, Writing and Emotion Tones from the Articles and appending them, along with their respective scores to the `.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the free form text response from the data frame\n",
    "# If you are using this script for a diff CSV, you will have to change this column name\n",
    "free_form_responses = df['Article Text']\n",
    "\n",
    "#Create a list to store the enriched data\n",
    "\n",
    "highestEmotionTone = []\n",
    "emotionToneScore = []\n",
    "\n",
    "languageToneScore = []\n",
    "highestLanguageTone = []\n",
    "\n",
    "socialToneScore = []\n",
    "highestSocialTone = []\n",
    "\n",
    "\n",
    "for response in free_form_responses:\n",
    "    if not response:\n",
    "        emotionToneScore.append(' ')\n",
    "        highestEmotionTone.append(' ')     \n",
    "        languageToneScore.append(' ')\n",
    "        highestLanguageTone.append(' ')     \n",
    "        socialToneScore.append(' ')\n",
    "        highestSocialTone.append(' ')  \n",
    "        continue\n",
    "\n",
    "    enriched_json = tone_analyzer.tone(text=response)\n",
    "    #print(enriched_json)\n",
    "        \n",
    "    if 'tone_categories' in enriched_json['document_tone']:\n",
    "        maxTone = max(enriched_json[\"document_tone\"][\"tone_categories\"][0][\"tones\"], key = itemgetter('score'))['tone_name']      \n",
    "        highestEmotionTone.append(maxTone)\n",
    "        maxToneScore = max(enriched_json[\"document_tone\"][\"tone_categories\"][0][\"tones\"], key = itemgetter('score'))['score']\n",
    "        emotionToneScore.append(maxToneScore)\n",
    "            \n",
    "        maxLanguageTone = max(enriched_json[\"document_tone\"][\"tone_categories\"][1][\"tones\"], key = itemgetter('score'))['tone_name']      \n",
    "        highestLanguageTone.append(maxLanguageTone)\n",
    "        maxLanguageScore = max(enriched_json[\"document_tone\"][\"tone_categories\"][1][\"tones\"], key = itemgetter('score'))['score']\n",
    "        languageToneScore.append(maxLanguageScore)\n",
    "            \n",
    "        maxSocial = max(enriched_json[\"document_tone\"][\"tone_categories\"][2][\"tones\"], key = itemgetter('score'))['tone_name']      \n",
    "        highestSocialTone.append(maxSocial)\n",
    "        maxSocialScore = max(enriched_json[\"document_tone\"][\"tone_categories\"][2][\"tones\"], key = itemgetter('score'))['score']\n",
    "        socialToneScore.append(maxSocialScore)\n",
    "    \n",
    "if highestEmotionTone:\n",
    "    df['articlehighestEmotionTone'] = highestEmotionTone    \n",
    "if emotionToneScore:\n",
    "    df['articleEmotionToneScore'] = emotionToneScore  \n",
    "if languageToneScore:\n",
    "    df['articlelanguageToneScore'] = languageToneScore\n",
    "if highestLanguageTone:\n",
    "    df['articlehighestLanguageTone'] = highestLanguageTone   \n",
    "if highestSocialTone:\n",
    "    df['articlehighestSocialTone'] = highestSocialTone    \n",
    "if socialToneScore:\n",
    "    df['articlesocialToneScore'] = socialToneScore \n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visual'></a> \n",
    "### 4.6 Visual Recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piclinks = df[\"Image\"]\n",
    "\n",
    "picclass = []\n",
    "\n",
    "for pic in piclinks:\n",
    "    # print(pic)\n",
    "    if not pic or pic == 'default-img':\n",
    "        picclass.append(' ')\n",
    "        continue\n",
    "\n",
    "    enriched_json = visual_recognition.classify(images_url=pic)\n",
    "    #print(enriched_json)\n",
    "    if 'error' in enriched_json:\n",
    "        print(enriched_json['error'])\n",
    "    if 'classifiers' in enriched_json['images'][0]:\n",
    "        classes = enriched_json['images'][0][\"classifiers\"][0][\"classes\"]\n",
    "        length = len(classes)\n",
    "        # print(classes)\n",
    "    else:\n",
    "        length = 0\n",
    "        \n",
    "    tpicclass = []\n",
    "    #for each class within one picture\n",
    "    for n in range(0,length):\n",
    "        #iclass is one class\n",
    "        iclass = classes[n]\n",
    "        #for confidence level .70\n",
    "        if float(iclass[\"score\"]>=.70):\n",
    "            tpicclass.append(iclass[\"class\"]) \n",
    "            \n",
    "    if(len(tpicclass) > 1):\n",
    "        tpicclass = \"\".join(reduce(lambda a, b: a + ', ' + b, tpicclass))\n",
    "    elif(len(tpicclass) == 0):\n",
    "        tpicclass = \"\"\n",
    "    else:\n",
    "        tpicclass = \"\".join(reduce(lambda a, b: a + ', ' + b, tpicclass))\n",
    "\n",
    "    picclass.append(tpicclass)\n",
    "\n",
    "df[\"PicClass\"] = picclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='write'></a>\n",
    "## Enrichment is now COMPLETE!\n",
    "<a id='write1'></a> \n",
    "Save a copy of the enriched DataFrame as a file in Bluemix Object Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_file(credentials, local_file_name):\n",
    "    \"\"\"Put the file in Bluemix Object Storage.\"\"\"\n",
    "    f = open(local_file_name,'r',encoding=\"utf-8\")\n",
    "    my_data = f.read()\n",
    "    data_to_send = my_data.encode(\"utf-8\")\n",
    "    url1 = ''.join(['https://identity.open.softlayer.com', '/v3/auth/tokens'])\n",
    "    data = {'auth': {'identity': {'methods': ['password'],\n",
    "            'password': {'user': {'name': credentials['username'],'domain': {'id': credentials['domain_id']},\n",
    "            'password': credentials['password']}}}}}\n",
    "    headers1 = {'Content-Type': 'application/json'}\n",
    "    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n",
    "    resp1_body = resp1.json()\n",
    "    for e1 in resp1_body['token']['catalog']:\n",
    "        if(e1['type']=='object-store'):\n",
    "            for e2 in e1['endpoints']:\n",
    "                        if(e2['interface']=='public'and e2['region']=='dallas'):\n",
    "                            url2 = ''.join([e2['url'],'/', credentials['container'], '/', local_file_name])\n",
    "    s_subject_token = resp1.headers['x-subject-token']\n",
    "    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n",
    "    resp2 = requests.put(url=url2, headers=headers2, data = data_to_send )\n",
    "    resp2.raise_for_status()\n",
    "    print('%s saved to Bluemix Object Storage. Status code=%s.' % (localfilename, resp2.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the enriched file name from the original filename.\n",
    "localfilename = 'enriched_' + credentials['filename']\n",
    "\n",
    "# Write a CSV file from the enriched pandas DataFrame.\n",
    "df.to_csv(localfilename, index=False)\n",
    "\n",
    "# Use the above put_file method with credentials to put the file in Object Storage.\n",
    "put_file(credentials, localfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a> \n",
    "# Part II - Data Preparation\n",
    "<a id='prepare'></a>\n",
    "## 1. Prepare Data\n",
    " <a id='visualizations'></a>\n",
    "### 1.1 Prepare Multiple DataFrames for Visualizations\n",
    "Before we can create the separate tables for each Watson feature we need to organize and reformat the data. First, we need to determine which data points are tied to metrics. Second, we need to make sure make sure each metric is numeric. _(This is necessary for PixieDust in Part III)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine which data points are tied to metrics and put them in a list\n",
    "metrics = [\"Lifetime Post Total Reach\", \"Lifetime Post organic reach\", \"Lifetime Post Paid Reach\", \"Lifetime Post Total Impressions\", \"Lifetime Post Organic Impressions\", \n",
    "           \"Lifetime Post Paid Impressions\", \"Lifetime Engaged Users\", \"Lifetime Post Consumers\", \"Lifetime Post Consumptions\", \"Lifetime Negative feedback\", \"Lifetime Negative Feedback from Users\", \n",
    "           \"Lifetime Post Impressions by people who have liked your Page\", \"Lifetime Post reach by people who like your Page\", \"Lifetime Post Paid Impressions by people who have liked your Page\", \n",
    "           \"Lifetime Paid reach of a post by people who like your Page\", \"Lifetime People who have liked your Page and engaged with your post\", \"Lifetime Talking About This (Post) by action type - comment\", \n",
    "           \"Lifetime Talking About This (Post) by action type - like\", \"Lifetime Talking About This (Post) by action type - share\", \"Lifetime Post Stories by action type - comment\", \"Lifetime Post Stories by action type - like\", \n",
    "           \"Lifetime Post Stories by action type - share\", \"Lifetime Post consumers by type - link clicks\", \"Lifetime Post consumers by type - other clicks\", \"Lifetime Post consumers by type - photo view\", \"Lifetime Post Consumptions by type - link clicks\", \n",
    "           \"Lifetime Post Consumptions by type - other clicks\", \"Lifetime Post Consumptions by type - photo view\", \"Lifetime Negative feedback - hide_all_clicks\", \"Lifetime Negative feedback - hide_clicks\", \n",
    "           \"Lifetime Negative Feedback from Users by Type - hide_all_clicks\", \"Lifetime Negative Feedback from Users by Type - hide_clicks\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tone'></a> \n",
    "### 1.2 Create a Consolidated Tone DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Tone DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list with only Post Tone Values\n",
    "post_tones = [\"Text\",\"highestEmotionTone\", \"emotionToneScore\", \"languageToneScore\", \"highestLanguageTone\", \"highestSocialTone\", \"socialToneScore\"]\n",
    "\n",
    "#Append DataFrame with these metrics\n",
    "post_tones.extend(metrics)\n",
    "\n",
    "#Create a new DataFrame with tones and metrics\n",
    "df_post_tones = df[post_tones]\n",
    "\n",
    "#Determine which tone values are suppose to be numeric and ensure they are numeric. \n",
    "post_numeric_values = [\"emotionToneScore\", \"languageToneScore\", \"socialToneScore\"]\n",
    "for i in post_numeric_values:\n",
    "    df_post_tones[i] = pd.to_numeric(df_post_tones[i], errors='coerce')\n",
    "\n",
    "#Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_post_tones[i] = pd.to_numeric(df_post_tones[i], errors='coerce')\n",
    "    \n",
    "#Drop NA Values in Tone Enrichment Columns\n",
    "df_post_tones.dropna(subset=[\"socialToneScore\"] , inplace = True)\n",
    "\n",
    "\n",
    "#Add in a column to distinguish what portion the enrichment was happening \n",
    "df_post_tones[\"Type\"] = \"Post\"\n",
    "\n",
    "# df_post_tones.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Article Tone DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list with only Article Tone Values\n",
    "article_tones = [\"Text\", \"articlehighestEmotionTone\", \"articleEmotionToneScore\", \"articlelanguageToneScore\", \"articlehighestLanguageTone\", \"articlehighestSocialTone\", \"articlesocialToneScore\"]\n",
    "\n",
    "#Append DataFrame with these metrics\n",
    "article_tones.extend(metrics)\n",
    "\n",
    "#Create a new DataFrame with tones and metrics\n",
    "df_article_tones = df[article_tones]\n",
    "\n",
    "#Determine which values are suppose to be numeric and ensure they are numeric. \n",
    "art_numeric_values = [\"articleEmotionToneScore\", \"articlelanguageToneScore\", \"articlesocialToneScore\"]\n",
    "for i in art_numeric_values:\n",
    "    df_article_tones[i] = pd.to_numeric(df_article_tones[i], errors='coerce')\n",
    "    \n",
    "#Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_article_tones[i] = pd.to_numeric(df_article_tones[i], errors='coerce')\n",
    "    \n",
    "#Drop NA Values in Tone Enrichment Columns\n",
    "df_article_tones.dropna(subset=[\"articlesocialToneScore\"] , inplace = True)\n",
    "\n",
    "#Add in a column to distinguish what portion the enrichment was happening \n",
    "df_article_tones[\"Type\"] = \"Article\"\n",
    "\n",
    "# df_article_tones.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Post and Article DataFrames to Make One Tone DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first make the Column Headers the same\n",
    "df_post_tones.rename(columns={\"highestEmotionTone\":\"Emotion Tone\", \"emotionToneScore\":\"Emotion Tone Score\", \"languageToneScore\": \"Language Tone Score\", \"highestLanguageTone\": \"Language Tone\", \"highestSocialTone\": \"Social Tone\", \"socialToneScore\":\"Social Tone Score\"\n",
    "}, inplace=True)\n",
    "\n",
    "df_article_tones.rename(columns={\"articlehighestEmotionTone\":\"Emotion Tone\", \"articleEmotionToneScore\":\"Emotion Tone Score\", \"articlelanguageToneScore\": \"Language Tone Score\", \"articlehighestLanguageTone\": \"Language Tone\", \"articlehighestSocialTone\": \"Social Tone\", \"articlesocialToneScore\":\"Social Tone Score\"\n",
    "}, inplace=True)\n",
    "\n",
    "#Combine into one data frame\n",
    "df_tones = pd.concat([df_post_tones, df_article_tones])\n",
    "\n",
    "#df_tones.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='keyword'></a> \n",
    "### 1.3 Create a Consolidated Keyword DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Article Keyword DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list with only Article Keywords\n",
    "article_keywords = [\"Text\", \"MaxLinkKeywords\"]\n",
    "\n",
    "#Append DataFrame with these metrics\n",
    "article_keywords.extend(metrics)\n",
    "\n",
    "#Create a new DataFrame with keywords and metrics\n",
    "df_article_keywords = df[article_keywords]\n",
    "\n",
    "#Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_article_keywords[i] = pd.to_numeric(df_article_keywords[i], errors='coerce')\n",
    "  \n",
    "#Drop NA Values in Keywords Column\n",
    "\n",
    "df_article_keywords['MaxLinkKeywords'].replace(' ', np.nan, inplace=True)\n",
    "df_article_keywords.dropna(subset=['MaxLinkKeywords'], inplace=True)\n",
    "\n",
    "#Add in a column to distinguish what portion the enrichment was happening \n",
    "df_article_keywords[\"Type\"] = \"Article\"\n",
    "\n",
    "#df_article_keywords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thumbnail Keyword DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list with only Thumbnail Keywords\n",
    "thumbnail_keywords = [\"Text\", \"MaxThumbnailKeywords\"]\n",
    "\n",
    "#Append DataFrame with these metrics\n",
    "thumbnail_keywords.extend(metrics)\n",
    "\n",
    "#Create a new DataFrame with keywords and metrics\n",
    "df_thumbnail_keywords = df[thumbnail_keywords]\n",
    "\n",
    "\n",
    "#Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_thumbnail_keywords[i] = pd.to_numeric(df_thumbnail_keywords[i], errors='coerce')\n",
    "    \n",
    "#Drop NA Values in Keywords Column\n",
    "\n",
    "df_thumbnail_keywords['MaxThumbnailKeywords'].replace(' ', np.nan, inplace=True)\n",
    "df_thumbnail_keywords.dropna(subset=['MaxThumbnailKeywords'], inplace=True)\n",
    "\n",
    "#Add in a column to distinguish what portion the enrichment was happening \n",
    "df_thumbnail_keywords[\"Type\"] = \"Thumbnails\"\n",
    "\n",
    "#df_thumbnail_keywords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Keyword DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list with only Thumbnail Keywords\n",
    "post_keywords = [\"Text\", \"MaxTextKeywords\"]\n",
    "\n",
    "#Append DataFrame with these metrics\n",
    "post_keywords.extend(metrics)\n",
    "\n",
    "#Create a new DataFrame with keywords and metrics\n",
    "df_post_keywords = df[post_keywords]\n",
    "\n",
    "#Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_post_keywords[i] = pd.to_numeric(df_post_keywords[i], errors='coerce')\n",
    "    \n",
    "#Drop NA Values in Keywords Column\n",
    "\n",
    "df_post_keywords['MaxTextKeywords'].replace(' ', np.nan, inplace=True)\n",
    "df_post_keywords.dropna(subset=['MaxTextKeywords'], inplace=True)\n",
    "\n",
    "#Add in a column to distinguish what portion the enrichment was happening \n",
    "df_post_keywords[\"Type\"] = \"Posts\"\n",
    "\n",
    "# df_post_keywords.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list with only Visual Recognition classes\n",
    "pic_keywords = [\"Text\", \"PicClass\"]\n",
    "\n",
    "#Append DataFrame with these metrics\n",
    "pic_keywords.extend(metrics)\n",
    "\n",
    "#Create a new DataFrame with keywords and metrics\n",
    "df_pic_keywords = df[pic_keywords]\n",
    "\n",
    "#Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_pic_keywords[i] = pd.to_numeric(df_pic_keywords[i], errors='coerce')\n",
    "    \n",
    "#Drop NA Values in Keywords Column\n",
    "\n",
    "df_pic_keywords['PicClass'].replace(' ', np.nan, inplace=True)\n",
    "df_pic_keywords.dropna(subset=['PicClass'], inplace=True)\n",
    "\n",
    "#Add in a column to distinguish what portion the enrichment was happening \n",
    "df_pic_keywords[\"Type\"] = \"Posts\"\n",
    "\n",
    "# print(df_pic_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Post, Thumbnail, and Article DataFrames to Make One Keywords DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first make the Column Headers the same\n",
    "df_post_keywords.rename(columns={\"MaxTextKeywords\": \"Keywords\"}, inplace=True)\n",
    "\n",
    "df_thumbnail_keywords.rename(columns={\"MaxThumbnailKeywords\":\"Keywords\"}, inplace=True)\n",
    "\n",
    "df_article_keywords.rename(columns={\"MaxLinkKeywords\":\"Keywords\"}, inplace=True)\n",
    "\n",
    "#Combine into one data frame\n",
    "df_keywords = pd.concat([df_post_keywords, df_thumbnail_keywords, df_article_keywords])\n",
    "\n",
    "#df_keywords = df_keywords[df_keywords[\"Lifetime Post Consumptions\"]>700]\n",
    "# print(df_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='entity'></a>\n",
    "###  1.4 Create a Consolidated Entity DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Article Entity DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list with only Article Keywords\n",
    "article_entities = [\"Text\", \"MaxLinkEntity\"]\n",
    "\n",
    "#Append DataFrame with these metrics\n",
    "article_entities.extend(metrics)\n",
    "\n",
    "#Create a new DataFrame with keywords and metrics\n",
    "df_article_entities = df[article_entities]\n",
    "    \n",
    "#Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_article_entities[i] = pd.to_numeric(df_article_entities[i], errors='coerce')\n",
    "    \n",
    "#Drop NA Values in Keywords Column\n",
    "\n",
    "df_article_entities['MaxLinkEntity'] = df[\"MaxLinkEntity\"].replace(r'\\s+', np.nan, regex=True)\n",
    "df_article_entities.dropna(subset=['MaxLinkEntity'], inplace=True)\n",
    "\n",
    "#Add in a column to distinguish what portion the enrichment was happening \n",
    "df_article_entities[\"Type\"] = \"Article\"\n",
    "\n",
    "#df_article_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thumbnail Entity DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list with only Thumbnail Keywords\n",
    "thumbnail_entities = [\"Text\", \"MaxThumbnailEntity\"]\n",
    "\n",
    "#Append DataFrame with these metrics\n",
    "thumbnail_entities.extend(metrics)\n",
    "\n",
    "#Create a new DataFrame with keywords and metrics\n",
    "df_thumbnail_entities = df[thumbnail_entities]\n",
    "\n",
    "#Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_thumbnail_entities[i] = pd.to_numeric(df_thumbnail_entities[i], errors='coerce')\n",
    "    \n",
    "#Drop NA Values in Keywords Column\n",
    "\n",
    "df_thumbnail_entities['MaxThumbnailEntity'] = df_thumbnail_entities['MaxThumbnailEntity'].replace(r'\\s+', np.nan, regex=True)\n",
    "df_thumbnail_entities.dropna(subset=['MaxThumbnailEntity'], inplace=True)\n",
    "\n",
    "#Add in a column to distinguish what portion the enrichment was happening \n",
    "df_thumbnail_entities[\"Type\"] = \"Thumbnails\"\n",
    "\n",
    "#df_thumbnail_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Entity DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list with only Thumbnail Keywords\n",
    "post_entities = [\"Text\", \"MaxTextEntity\"]\n",
    "\n",
    "#Append DataFrame with these metrics\n",
    "post_entities.extend(metrics)\n",
    "\n",
    "#Create a new DataFrame with keywords and metrics\n",
    "df_post_entities = df[post_entities]\n",
    "\n",
    "#Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_post_entities[i] = pd.to_numeric(df_post_entities[i], errors='coerce')\n",
    "    \n",
    "#Drop NA Values in Keywords Column\n",
    "\n",
    "df_post_entities['MaxTextEntity'] = df_post_entities['MaxTextEntity'].replace(r'\\s+', np.nan, regex=True)\n",
    "df_post_entities.dropna(subset=['MaxTextEntity'], inplace=True)\n",
    "\n",
    "#Add in a column to distinguish what portion the enrichment was happening \n",
    "df_post_entities[\"Type\"] = \"Posts\"\n",
    "\n",
    "#df_post_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Post, Thumbnail, and Article DataFrames to Make One Entity DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first make the Column Headers the same\n",
    "df_post_entities.rename(columns={\"MaxTextEntity\": \"Entities\"}, inplace=True)\n",
    "\n",
    "df_thumbnail_entities.rename(columns={\"MaxThumbnailEntity\":\"Entities\"}, inplace=True)\n",
    "\n",
    "df_article_entities.rename(columns={\"MaxLinkEntity\":\"Entities\"}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#Combine into one data frame\n",
    "df_entities = pd.concat([df_post_entities, df_thumbnail_entities, df_article_entities])\n",
    "\n",
    "df_entities[\"Entities\"] = df_entities[\"Entities\"].replace('', np.nan)\n",
    "df_entities.dropna(subset=[\"Entities\"], inplace=True)\n",
    "\n",
    "\n",
    "#df_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3\"></a> \n",
    "# Part III\n",
    "<a id='2setup'></a> \n",
    "## 1. Setup\n",
    "<a id='2setup2'></a>\n",
    "###  1.1 Assign Variables\n",
    "Assign new DataFrames to variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = df_entities\n",
    "tones = df_tones\n",
    "keywords = df_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2visual'></a>\n",
    "##  2. Visualize Data\n",
    "<a id='2visual1'></a> \n",
    "### 2.1 Run PixieDust Visualization Library with Display() API\n",
    "PixieDust lets you visualize your data in just a few clicks using the display() API. You can find more info at https://ibm-cds-labs.github.io/pixiedust/displayapi.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can use a pie chart to identify how post consumption was broken up by tone. \n",
    "\n",
    "Click on the `Options` button to change the chart.  Here are some things to try:\n",
    "* Add *Type* to make the breakdown show *Post* or *Article*.\n",
    "* Show *Social Tone* intead of *Emotion Tone* (or both).\n",
    "* Try a different metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "chartsize": "70",
      "handlerId": "pieChart",
      "keyFields": "Emotion Tone",
      "legend": "false",
      "mpld3": "false",
      "rowCount": "100",
      "title": "Tone in Posts and Articles",
      "valueFields": "Lifetime Post Consumptions"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(tones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at the same statistics as a bar chart.\n",
    "\n",
    "It is the same line of code. Use the `Edit Metadata` button to see how PixieDust knows to show us a bar chart. If you don't have a button use the menu and select `View > Cell Toolbar > Edit Metadata`.\n",
    "\n",
    "A bar chart is better at showing more information. We added `Cluster By: Type` so we already see numbers for posts and articles. Notice what the chart tells you. *Joy* seems to be the dominant emotion. Click on `Options` and try this:\n",
    "\n",
    "* Change the aggregation to `AVG`.\n",
    "\n",
    "What emotion leads to higher average consumption?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "chartsize": "70",
      "charttype": "grouped",
      "clusterby": "Type",
      "handlerId": "barChart",
      "keyFields": "Emotion Tone",
      "legend": "true",
      "rendererId": "matplotlib",
      "rowCount": "100",
      "title": "Tone in Posts and Articles",
      "valueFields": "Lifetime Post Consumptions"
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(tones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at the entities that were detected by Natural Language Understanding.\n",
    "\n",
    "The following bar chart shows the entities that were detected. This time we are stacking negative feedback and \"likes\" to get a picture of the kind of feedback the entities were getting. We chose a horizontal, stacked bar chart with descending values for a little variety.\n",
    "\n",
    "* Try a different renderer and see what you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "chartsize": "70",
      "charttype": "stacked",
      "handlerId": "barChart",
      "keyFields": "Entities",
      "orientation": "horizontal",
      "rendererId": "matplotlib",
      "rowCount": "100",
      "sortby": "Values ASC",
      "title": "Entities in Posts and Articles",
      "valueFields": "Lifetime Post Stories by action type - like,Lifetime Negative Feedback from Users"
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we look at the keywords detected by Natural Language Understanding\n",
    "\n",
    "This time we are using the bokeh renderer for a different look. The renderers have different capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "AVG",
      "chartsize": "99",
      "charttype": "stacked",
      "clusterby": "Type",
      "handlerId": "barChart",
      "keyFields": "Keywords",
      "legend": "true",
      "mpld3": "false",
      "orientation": "horizontal",
      "rendererId": "bokeh",
      "rowCount": "40",
      "sortby": "Values ASC",
      "timeseries": "false",
      "title": "Keywords in Posts, Articles and Thumbnails",
      "valueFields": "Lifetime Post Consumptions"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's take a look at what Visual Recognition can show us.\n",
    "\n",
    "See how the images influenced the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "AVG",
      "chartsize": "99",
      "handlerId": "barChart",
      "keyFields": "PicClass",
      "legend": "false",
      "rowCount": "500",
      "sortby": "Values ASC",
      "title": "Images",
      "valueFields": "Lifetime Post Consumptions"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(df_pic_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Info.\n",
    "For more information about PixieDust check out the following:\n",
    "#### PixieDust Documentation: https://ibm-cds-labs.github.io/pixiedust/index.html\n",
    "#### PixieDust GitHub Repo: https://github.com/ibm-cds-labs/pixiedust"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3.5 (Experimental) with Spark 2.0",
   "language": "python",
   "name": "python3-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
